{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Chat Studio Developer Documentation","text":"<p>Welcome to the Open Chat Studio developer documentation. This guide is designed to help you understand the architecture, setup your development environment, and contribute to the project effectively.</p>"},{"location":"#what-is-open-chat-studio","title":"What is Open Chat Studio?","text":"<p>Open Chat Studio is a platform for building, deploying, and evaluating AI-powered chat experiences. It provides tools for working with various LLMs (Language Learning Models), creating chatbots, managing conversations, and integrating with different messaging platforms.</p> <p>For user facing documentation, please visit docs.openchatstudio.com.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Create LLM-based based chatbots</li> <li>Create and manage conversational agents</li> <li>Connect to various messaging platforms</li> <li>Track and analyze conversations</li> <li>Customize conversation flows and logic</li> <li>Integrate with various service providers</li> </ul>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Contributing: Guidelines for contributing to the project</li> <li>Getting Started: Setup your development environment</li> </ul>"},{"location":"admin_guides/","title":"Admin Guides","text":"<p>This section contains documentation for features that are typically used by system administrators and superusers to manage the Open Chat Studio platform.</p>"},{"location":"admin_guides/#available-guides","title":"Available Guides","text":"<ul> <li>Banners System - Create and manage system-wide notification banners</li> <li>Feature Flags - Manage access to feature flags</li> </ul>"},{"location":"admin_guides/#overview","title":"Overview","text":"<p>Admin features in Open Chat Studio are designed to help system administrators:</p> <ul> <li>Communicate important information to users</li> <li>Manage platform-wide settings and configurations</li> <li>Monitor and maintain system health</li> <li>Control feature availability and access</li> </ul> <p>Most admin features require superuser privileges and are accessible through the Django admin interface.</p>"},{"location":"admin_guides/banners/","title":"Banners System","text":"<p>The banners system allows system administrators to create and manage notification banners that appear to users throughout the Open Chat Studio platform.</p>"},{"location":"admin_guides/banners/#overview","title":"Overview","text":"<p>Banners are temporary notifications that can be displayed to users on specific pages or globally across the entire platform. They support different visual styles and can be scheduled to appear and disappear automatically.</p>"},{"location":"admin_guides/banners/#features","title":"Features","text":"<ul> <li>Multiple Banner Types: Information, warning, error, and success banners</li> <li>Location Targeting: Display banners on specific pages or globally as well as on specific sites (domains) or all sites</li> <li>Scheduling: Set start and end dates for automatic banner display</li> <li>Feature Flag Integration: Show banners only to teams with specific feature flags</li> <li>User Dismissal: Users can dismiss banners with optional re-appearance timeout</li> <li>Markdown Support: Banner messages support markdown formatting</li> <li>Template Variables: Dynamic content using Django template syntax</li> </ul>"},{"location":"admin_guides/banners/#creating-banners","title":"Creating Banners","text":""},{"location":"admin_guides/banners/#access-the-admin-interface","title":"Access the Admin Interface","text":"<ol> <li>Navigate to the Django admin interface (typically <code>/admin/</code>)</li> <li>Log in with superuser credentials</li> <li>Find the \"Banners\" section and click \"Banners\"</li> <li>Click \"Add banner\" to create a new banner</li> </ol>"},{"location":"admin_guides/banners/#banner-configuration","title":"Banner Configuration","text":""},{"location":"admin_guides/banners/#basic-information","title":"Basic Information","text":"<ul> <li>Title (optional): A brief title for the banner</li> <li>Message: The main content displayed to users (supports markdown)</li> <li>Banner Type: Choose the visual style:</li> <li><code>info</code> - Blue information banner</li> <li><code>warning</code> - Yellow warning banner</li> <li><code>error</code> - Red error banner</li> <li><code>success</code> - Green success banner</li> </ul>"},{"location":"admin_guides/banners/#location-settings","title":"Location Settings","text":"<ul> <li>Location: Where the banner should appear:</li> <li><code>global</code> - All pages (default)</li> <li><code>pipelines</code> - Pipelines home page</li> <li><code>pipelines_new</code> - New pipeline creation page</li> <li><code>chatbots_home</code> - Chatbots listing page</li> <li><code>chatbots_new</code> - New chatbot creation page</li> <li><code>assistants_home</code> - Assistants listing page</li> <li><code>team_settings</code> - Team settings page</li> </ul>"},{"location":"admin_guides/banners/#scheduling","title":"Scheduling","text":"<ul> <li>Start Date: When the banner should first appear (defaults to current time)</li> <li>End Date: When the banner should stop appearing (required)</li> <li>Is Active: Manual toggle to enable/disable the banner</li> </ul>"},{"location":"admin_guides/banners/#advanced-options","title":"Advanced Options","text":"<ul> <li>Feature Flag: Only show the banner to teams that have this feature flag enabled</li> <li>Dismiss Timeout: Number of days before a dismissed banner reappears (0 = never reappear)</li> </ul>"},{"location":"admin_guides/banners/#banner-display-logic","title":"Banner Display Logic","text":""},{"location":"admin_guides/banners/#visibility-rules","title":"Visibility Rules","text":"<p>A banner is visible when ALL of the following conditions are met:</p> <ol> <li>The banner is marked as active (<code>is_active = True</code>)</li> <li>The current time is between the start and end dates</li> <li>The user hasn't dismissed the banner (or the dismiss timeout has expired)</li> <li>The banner location matches the current page (or is set to \"global\")</li> <li>If a feature flag is set, the user's team must have that flag enabled</li> </ol>"},{"location":"admin_guides/banners/#display-locations","title":"Display Locations","text":"<p>The banner location is determined by the <code>BannerLocationMiddleware</code> which maps URL patterns to banner locations:</p> <ul> <li>Global banners appear on all pages</li> <li>Location-specific banners only appear on their designated pages</li> <li>Multiple banners can be active simultaneously</li> </ul>"},{"location":"admin_guides/banners/#message-formatting","title":"Message Formatting","text":""},{"location":"admin_guides/banners/#markdown-support","title":"Markdown Support","text":"<p>Banner messages support standard markdown formatting:</p> <pre><code>**Bold text** and *italic text*\n[Links](https://example.com)\n- Bullet points\n- More bullets\n</code></pre>"},{"location":"admin_guides/banners/#template-variables","title":"Template Variables","text":"<p>You can use Django template variables in banner messages:</p> <pre><code>Welcome back, {{ request.user.first_name }}!\nCheck out this new feature: &lt;a href=\"{% url \"cool-feature\" request.team.slug %}\"&gt;GO!&lt;/a&gt;.\n</code></pre> <p>Note: Template errors are only shown to superusers for security reasons.</p>"},{"location":"admin_guides/banners/#user-interaction","title":"User Interaction","text":""},{"location":"admin_guides/banners/#dismissing-banners","title":"Dismissing Banners","text":"<p>Users can dismiss banners by clicking the dismiss button (\u00d7). When dismissed:</p> <ul> <li>A cookie is set to remember the dismissal</li> <li>The banner won't reappear until the dismiss timeout expires</li> <li>The cookie expires when the banner ends or after the timeout period</li> </ul>"},{"location":"admin_guides/banners/#dismiss-timeout-behavior","title":"Dismiss Timeout Behavior","text":"<ul> <li>0 days: Banner never reappears once dismissed</li> <li>N days: Banner reappears N days after dismissal</li> <li>Cookie expires at the earlier of: banner end date or dismiss timeout</li> </ul>"},{"location":"admin_guides/feature_flags/","title":"Feature Flags","text":"<p>Feature flags allow you to toggle features on/off for specific teams without code deployments. For technical details on how flags are created and used in code see the developer guide.</p> <p>Access to feature flags is managed via a custom page in OCS which is only accessible to users with the 'staff' or 'superuser' permission. Users with those permissions will see a 'Feature Flags' menu item at the bottom of the left side menu.</p> <p>Feature flags can be activated for:</p> <ul> <li>Everyone (all users)</li> <li>Superusers</li> <li>Specific teams</li> <li>Specific users</li> </ul> <p>If any of these matches for a user, they will have access to that feature flag.</p> <p>There are also two special modes:</p> <ul> <li>Testing mode</li> <li>This allows activating a flag using a URL parameter </li> <li>Rollout mode</li> <li>This allows activating a flag for a percentage of users. </li> </ul>"},{"location":"agents/django_model_auditing/","title":"Django model change auditing","text":"<p>This project uses <code>django-field-audit</code> to track changes to specific Django models.</p> <p>See the <code>apps.experiments.Experiment</code> for an example.</p> <p>The basic pattern is as follows:</p> <pre><code>from apps.audit.decorators import audit_fields\n\nclass MyModelManager(AuditingManager):\n    pass\n\n@audit_fields(\"field_a\", \"field_b\", audit_special_queryset_writes=True)\nclass MyModel(BaseTeamModel):\n    # Define audit fields in model_audit_fields.py\n    objects = MyModelManager()\n</code></pre>"},{"location":"agents/django_model_versioning/","title":"Versioning System","text":"<p>This project includes a custom data versioning system to allow users to version their chatbots.</p> <p>The main entry point for this is <code>apps.experiments.Experiment.create_new_version</code>.</p> <p>See the developer docs</p>"},{"location":"agents/django_performance/","title":"Performance Patterns","text":""},{"location":"agents/django_performance/#lazy-loading-heavy-imports","title":"Lazy Loading Heavy Imports","text":"<p>Avoid importing heavy AI/ML libraries at module level to keep Django startup time fast: <pre><code># \u274c BAD - imports at module level (slow startup)\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_anthropic import ChatAnthropic\n\ndef get_model():\n    return ChatVertexAI(...)\n\n# \u2705 GOOD - lazy import inside method (fast startup)\ndef get_model():\n    from langchain_google_vertexai import ChatVertexAI\n    return ChatVertexAI(...)\n</code></pre></p> <p>Heavy libraries that benefit from lazy loading: * <code>langchain_google_vertexai</code> (~45s import time) * <code>langchain_google_genai</code> (~9s import time) * <code>langchain_anthropic</code>, <code>langchain_openai</code> (~3s combined) * <code>boto3</code>, <code>pandas</code>, <code>numpy</code> (when not always needed)</p> <p>Use <code>TYPE_CHECKING</code> for type hints only: <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from langchain_google_vertexai import ChatVertexAI\n</code></pre></p>"},{"location":"agents/django_view_security/","title":"Django View Security Patterns","text":"<p>Always use team-based security: <pre><code>from apps.teams.decorators import login_and_team_required\nfrom django.contrib.auth.decorators import permission_required\n\n# Function-based views\n@login_and_team_required\n@permission_required(\"my_app.view_mymodel\")\ndef my_view(request, team_slug: str):\n    current_user = request.user\n    current_team = request.team\n    team_membership = request.team_membership\n    pass\n\n# Class-based views\nfrom apps.teams.mixins import LoginAndTeamRequiredMixin\nfrom django.contrib.auth.mixins import PermissionRequiredMixin\n\nclass MyView(LoginAndTeamRequiredMixin, PermissionRequiredMixin, View):\n    permission_required = \"my_app.view_mymodel\"\n</code></pre></p>"},{"location":"agents/multi_tenancy/","title":"Team-Based Multi-Tenancy","text":"<ul> <li>All data is scoped to teams via <code>BaseTeamModel</code></li> <li>Use <code>@login_and_team_required</code> or <code>@team_required</code> decorators on views</li> <li>Team context available in middleware as <code>request.team</code> and <code>request.team_membership</code></li> <li>Permission system based on team membership</li> <li>Never allow cross-team data access without explicit permission</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This section provides an overview of the Open Chat Studio architecture, explaining the core concepts and components that make up the system.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Open Chat Studio is built as a Django web application with a modular design. It consists of several Django apps that handle different aspects of the system.</p>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":"<ul> <li>Backend: Django, Django REST Framework, Celery</li> <li>Database: PostgreSQL</li> <li>Cache/Message Broker: Redis</li> <li>Frontend: HTML, CSS (Tailwind + DaisyUI), htmx, AlpineJS, ReactJS with React Flow (for specific components)</li> <li>External Services: OpenAI, Azure, etc.</li> </ul>"},{"location":"architecture/#key-concepts","title":"Key Concepts","text":""},{"location":"architecture/#experiments","title":"Experiments","text":"<p>Experiments are configurations for AI chat experiences. They include: - Prompts and LLM configurations - Channel connections - Data collection settings</p> <p>Note</p> <p>The term 'Experiments' is a legacy term. On the user interface side, they are referred to as 'Chatbots'.</p>"},{"location":"architecture/#channels","title":"Channels","text":"<p>Channels are communication interfaces that connect users to the chat system. These include: - Web chat - Slack - WhatsApp - Facebook Messenger - Custom integrations</p>"},{"location":"architecture/#service-providers","title":"Service Providers","text":"<p>Service providers enable integration with external services: - LLM providers (OpenAI, Azure, etc.) - Voice providers - Authentication providers - Messaging providers - Tracing providers</p>"},{"location":"architecture/#pipelines","title":"Pipelines","text":"<p>Pipelines allow for the creation of complex workflows with multiple nodes and processing steps.</p>"},{"location":"architecture/#project-structure","title":"Project structure","text":"<p>The project is organized into several Django apps, each responsible for a specific functionality. Apps are placed in the <code>apps</code> folder, and each app has its own models, views, serializers, and tests. </p>"},{"location":"architecture/#django-templates","title":"Django Templates","text":"<p>Templates as well as static files are centralized in the <code>templates</code> and <code>assets</code> folders, respectively. Templates specific to an app should be placed in the <code>templates/{app_name}</code> directory.</p>"},{"location":"architecture/#static-files","title":"Static Files","text":"<p>The <code>assets</code> folder contains JavaScript, CSS. The <code>assets/styles</code> folder contains Tailwind CSS configurations, while the <code>assets/javascript</code> folder contains JavaScript modules. These files are processed and bundled using Webpack to create the final static assets served to users. Other static assets like images are placed directly in the <code>static/</code> folder.</p>"},{"location":"architecture/#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":""},{"location":"architecture/#background-tasks","title":"Background Tasks","text":"<p>Open Chat Studio uses Celery for asynchronous task processing, which is critical for handling LLM interactions, scheduled messages, and other background operations.</p> <p>Key Files:</p> <ul> <li><code>config/celery.py</code>: Celery configuration</li> <li>Various <code>tasks.py</code> files in different apps</li> </ul>"},{"location":"architecture/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>The system uses Django's authentication system along with custom middleware and decorators to ensure proper access control.</p> <p>Key Files:</p> <ul> <li><code>teams/middleware.py</code>: Team-based access control</li> <li><code>teams/decorators.py</code>: Permission decorators</li> </ul>"},{"location":"architecture/#frontend-framework","title":"Frontend Framework","text":"<p>The frontend uses a combination of Django templates, Tailwind CSS, and JavaScript to create a responsive and interactive user interface.</p> <p>Key Files:</p> <ul> <li><code>templates/</code>: HTML templates</li> <li><code>assets/styles/</code>: CSS and Tailwind configurations</li> <li><code>assets/javascript/</code>: JavaScript modules</li> </ul>"},{"location":"architecture/#external-services","title":"External services","text":"<p>Sentry</p> <ul> <li>Purpose: Error reporting and tracking  </li> <li>Used for: Identifying and debugging production issues</li> </ul>"},{"location":"architecture/#task-badger","title":"Task Badger","text":"<ul> <li>Purpose: Celery task monitoring  </li> <li>Used for: Monitoring asynchronous task execution and performance</li> </ul> <p>BetterStack</p> <ul> <li>Purpose: Uptime monitoring and status page  </li> <li>Status Page: status.openchatstudio.com </li> <li>Used for: Monitoring system availability and communicating status to users</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Open Chat Studio! This guide will help you understand the contribution process and code style conventions.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<p>Before contributing, please make sure you've set up your development environment according to the Getting Started guide.</p>"},{"location":"contributing/#forms-of-contribution","title":"Forms of Contribution","text":""},{"location":"contributing/#provide-feedback","title":"Provide Feedback","text":"<ul> <li> <p>Report Bugs / Issues   If you encounter any issues or unexpected behavior in Open Chat Studio or its components, you can create a new issue in the GitHub issue tracker.</p> </li> <li> <p>Propose New Features / Improvements   If you have a suggestion for improving Open Chat Studio or want to share your ideas, you can open a new GitHub Discussion. If your idea is well-defined, you can also create a Feature Request Issue.   Provide a detailed description, including use cases, benefits, and potential challenges. Even if your idea is not immediately prioritized, it may still be considered later or taken up by the community.</p> </li> </ul>"},{"location":"contributing/#contribute-code-changes","title":"Contribute Code Changes","text":"<ul> <li>Fix Bugs or Develop New Features   If you want to help improve Open Chat Studio's codebase, choose an issue from the GitHub Issue Tracker and create a Pull Request addressing it. If you are new, check out the Good First Issues.</li> </ul> <p>Before starting, ensure that the change has not already been implemented. You can build Open Chat Studio using the latest <code>main</code> branch and confirm that the modification is still needed. If the feature is complex, discuss it first in the GitHub Discussions.</p>"},{"location":"contributing/#improve-documentation","title":"Improve Documentation","text":"<ul> <li>Developer Documentation needs improvement, and we welcome contributions.</li> <li>User Documentation is maintained in the open-chat-studio-docs repository and published at docs.openchatstudio.com.</li> <li>The easiest way to contribute to documentation is by reviewing and providing feedback. If you notice errors or opportunities for improvement, reach out to documentation contributors or create a Pull Request directly.</li> </ul>"},{"location":"contributing/#technical-guide","title":"Technical Guide","text":"<p>This section provides the necessary steps to set up your environment, build Open Chat Studio locally, and run tests.</p>"},{"location":"contributing/#1-set-up-your-environment","title":"1. Set Up Your Environment","text":"<p>Before contributing, please make sure you've set up your development environment according to the Getting Started guide.</p>"},{"location":"contributing/#2-start-working-on-your-first-issue","title":"2. Start Working on Your First Issue","text":"<p>To contribute, pick a task from the Good First Issues board. To be assigned to an issue, leave a comment with the <code>.take</code> command in the selected issue.</p>"},{"location":"contributing/#3-submit-a-pull-request-pr","title":"3. Submit a Pull Request (PR)","text":"<p>Follow our Pull Request guidelines.</p>"},{"location":"contributing/#agent-support","title":"Agent support","text":"<p>If you are using an agent other than Claude, consider creating a symbolic link to the CLAUDE.md file, but for your agent. For instance, to create a symlink for Gemini, run</p> <pre><code>ln -s CLAUDE.md GEMINI.md\n</code></pre> <p>Be sure add the new \"file\" to .gitignore.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you have any questions or need assistance: - Use GitHub Discussions for general queries. - Check existing issues or open a new one if necessary. - Reach out to maintainers in GitHub if you need further guidance.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to Open Chat Studio, you agree that your contributions will be licensed under the terms stated in the LICENSE file.</p>"},{"location":"contributing/pull_requests/","title":"How to Prepare a Good Pull Request","text":"<p>Open Chat Studio is an open-source project, and you can contribute to its code directly. To do so, follow these guidelines for creating Pull Requests (PRs) to maximize the chances of your changes being merged.</p>"},{"location":"contributing/pull_requests/#general-rules-for-a-good-pull-request","title":"General Rules for a Good Pull Request","text":"<ul> <li>Fork the repository and use your fork to create PRs. Avoid creating change branches in the main repository.</li> <li>Choose an appropriate branch for your work and create your own branch based on it.</li> <li>Give your branches, commits, and Pull Requests meaningful names and descriptions. This helps track changes later. If your changes cover a particular component, indicate it in the PR name as a prefix, for example: <code>[DOCS] PR name</code>.</li> <li>Keep your PRs small\u2014each PR should address one issue. Remove all unrelated changes.</li> <li>Link your Pull Request to an issue if applicable.</li> <li>Document your contribution! If your changes impact how users interact with Open Chat Studio, update the relevant documentation. You can do this yourself or collaborate with documentation contributors.</li> <li>For Work In Progress or early test results, use a Draft PR.</li> </ul>"},{"location":"contributing/pull_requests/#ensure-change-quality","title":"Ensure Change Quality","text":"<p>Your pull request will automatically be tested and marked as \"green\" when it is ready for merging. If any builds fail (\"red\" status), you need to fix the issues listed in console logs. Any change to the PR branch will automatically trigger the checks, so you don't need to recreate the PR\u2014just wait for the updated results.</p> <p>Regardless of automated tests, ensure the quality of your changes:</p> <ul> <li>Test your changes locally:</li> <li>Double-check your code.</li> <li>Run tests locally to identify and fix potential issues.</li> <li>Before creating a PR, ensure your branch is up to date with the latest state of the branch you are contributing to (e.g. <code>git fetch upstream &amp;&amp; git merge upstream/master</code>).</li> </ul>"},{"location":"contributing/pull_requests/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create your PR in draft state.</li> <li>Add a clear description of your changes and link any related issues.</li> <li>Request AI review by commenting <code>@coderabbit review</code> on your PR.</li> <li>Address any AI-suggested improvements.</li> <li>When ready, change PR state to \"Ready for review\" and assign reviewers.</li> <li>Address reviewer feedback.</li> <li>Once approved, your changes can be merged.</li> </ol> <p>Note: The AI review stage is optional but recommended for larger changes.</p>"},{"location":"contributing/pull_requests/#test-policy","title":"Test Policy","text":"<ul> <li>We use <code>pytest</code> for unit testing.</li> <li>All changes should ideally include tests.</li> <li>Focus on testing business logic and complex functionality.</li> <li>Generally, we don't test Django views unless they contain significant logic.</li> <li>When view logic becomes complex, extract it into separate functions and test those.</li> </ul>"},{"location":"contributing/pull_requests/#documentation-policy","title":"Documentation Policy","text":"<ul> <li>User-facing changes should be accompanied by documentation updates in the docs repo.</li> <li>Link the docs PR to the code PR.</li> <li>Merge the docs PR after the code PR.</li> </ul> <p>See the user docs guide for more detail.</p>"},{"location":"contributing/pull_requests/#communication","title":"Communication","text":"<p>We use GitHub issues for most work and have a GitHub project where we prioritize and plan work: GitHub Project.</p> <p>General questions and discussions can be conducted in the GitHub Discussions section.</p>"},{"location":"contributing/pull_requests/#need-additional-help-check-these-articles","title":"Need Additional Help? Check These Articles","text":"<ul> <li>How to create a fork</li> <li>Install Git</li> </ul>"},{"location":"developer_guides/ai_development/","title":"Claude Code Development Workflow","text":"<p>This project uses Claude Code with specialized slash commands for structured development. This approach follows advanced context engineering principles using commands from dimagi-claude-plugins.</p>"},{"location":"developer_guides/ai_development/#five-step-development-flow","title":"Five-Step Development Flow","text":""},{"location":"developer_guides/ai_development/#1-research-the-codebase","title":"1. Research the Codebase","text":"<p>Command: <code>/research_codebase</code></p> <p>Before starting work, explore the codebase to find relevant code, patterns, files, and architecture related to your task.</p> <p>Output: Documentation written to <code>docs/claude/research/</code></p> <p>Purpose: Build context about existing implementations, coding patterns, and related functionality before planning changes.</p>"},{"location":"developer_guides/ai_development/#2-create-implementation-plan","title":"2. Create Implementation Plan","text":"<p>Command: <code>/create_plan</code></p> <p>Based on the task requirements and research findings, create a detailed implementation plan.</p> <p>Output: Plan written to <code>docs/claude/plans/</code></p> <p>Purpose: Define the approach, identify files to modify, and outline implementation steps before writing code.</p>"},{"location":"developer_guides/ai_development/#3-iterate-on-the-plan","title":"3. Iterate on the Plan","text":"<p>Command: <code>/iterate_plan</code></p> <p>Refine the plan based on feedback, additional research, or changed requirements.</p> <p>Purpose: Ensure the plan is comprehensive and accurate before implementation begins.</p>"},{"location":"developer_guides/ai_development/#4-implement-the-plan","title":"4. Implement the Plan","text":"<p>Command: <code>/implement_plan</code></p> <p>Execute the plan by writing code, running tests, and making the planned changes.</p> <p>Purpose: Systematically implement the solution following the validated plan.</p>"},{"location":"developer_guides/ai_development/#5-validate-implementation","title":"5. Validate Implementation","text":"<p>Command: <code>/validate_plan</code></p> <p>Verify that the implementation meets the plan's requirements and success criteria.</p> <p>Purpose: Confirm the implementation is complete and correct before considering the task done.</p>"},{"location":"developer_guides/ai_development/#benefits","title":"Benefits","text":"<ul> <li>Reduced errors: Research and planning before coding prevents architectural mistakes</li> <li>Better context: Claude understands the codebase before making changes</li> <li>Traceable decisions: Plans and research are documented for future reference</li> <li>Systematic approach: Structured workflow ensures nothing is missed</li> <li>Easier reviews: Documented plans make code reviews more focused</li> </ul>"},{"location":"developer_guides/ai_development/#tips","title":"Tips","text":"<ul> <li>Use <code>/research_codebase</code> even for small tasks to understand existing patterns</li> <li>Iterate on plans when requirements are unclear or complex</li> <li>Reference research documents when asking questions about implementation</li> <li>Keep plans focused on a single feature or fix</li> </ul>"},{"location":"developer_guides/ai_development/#local-environment-setup","title":"Local Environment Setup","text":"<p>To use these slash commands in Claude Code, install the required plugin:</p> <pre><code>/plugin marketplace add dimagi/claude-plugins\n/plugin install research-plan-build\n</code></pre>"},{"location":"developer_guides/common_practises/","title":"Common Practices","text":""},{"location":"developer_guides/common_practises/#rendering-tags-for-tagged-objects","title":"Rendering Tags for Tagged Objects","text":"<p>When displaying objects that inherit from <code>TaggedModelMixin</code>, always prefetch tag data to avoid N+1 queries. Use <code>object.prefetched_tags_json</code> in templates to access the prefetched tags.</p>"},{"location":"developer_guides/common_practises/#implementation","title":"Implementation","text":"<p>Query with prefetch: <pre><code>from django.db.models import Prefetch\n\nmessages_queryset = (\n    ChatMessage.objects.filter(chat=session.chat)\n    .prefetch_related(\n        Prefetch(\n            \"tagged_items\",\n            queryset=CustomTaggedItem.objects.select_related(\"tag\", \"user\"),\n            to_attr=\"prefetched_tagged_items\",  # Required attribute name\n        )\n    )\n)\n</code></pre></p>"},{"location":"developer_guides/common_practises/#key-points","title":"Key Points","text":"<ul> <li>Must use <code>to_attr=\"prefetched_tagged_items\"</code> - this exact name is required</li> <li>Include <code>select_related(\"tag\", \"user\")</code> for optimal performance</li> </ul>"},{"location":"developer_guides/common_practises/#using-tom-select-for-multiselect-inputs","title":"Using Tom Select for Multiselect Inputs","text":"<p>For rich multiselect UI elements, we use the Tom Select JavaScript library. It's versatile and can be configured for simple selection or for creating new items on-the-fly (like tags).</p>"},{"location":"developer_guides/common_practises/#initialization","title":"Initialization","text":"<p>To use it, you need an HTML element, typically a <code>&lt;select multiple&gt;</code>, and then you initialize the <code>TomSelect</code> object in your JavaScript, pointing it to that element. TomSelect is available globally on the <code>window</code> object as <code>window.TomSelect</code>.</p> <p>HTML: <pre><code>&lt;select id=\"teams-select\" multiple class=\"w-full\"&gt;&lt;/select&gt;\n</code></pre></p> <p>JavaScript: <pre><code>const selectElement = document.getElementById(\"teams-select\");\nconst teamsSelect = new TomSelect(selectElement, {\n    plugins: ['remove_button'],\n    maxItems: null,\n});\n</code></pre></p>"},{"location":"developer_guides/custom_migrations/","title":"Custom Migrations","text":"<p>The custom migrations system tracks data migrations that run outside Django's standard migration framework, ensuring they execute exactly once across all environments.</p>"},{"location":"developer_guides/custom_migrations/#usage","title":"Usage","text":""},{"location":"developer_guides/custom_migrations/#management-command","title":"Management Command","text":"<p>Create a command that inherits from <code>IdempotentCommand</code>:</p> <pre><code>from apps.data_migrations.management.commands.base import IdempotentCommand\n\nclass Command(IdempotentCommand):\n    help = \"Migrate user data to new format\"\n    migration_name = \"migrate_user_data_v1_2024_11_21\"\n\n    def perform_migration(self, dry_run=False):\n        users = User.objects.filter(needs_migration=True)\n\n        if dry_run:\n            self.stdout.write(f\"Would update {users.count()} users\")\n            return\n\n        updated = users.update(migrated=True)\n        return updated\n</code></pre>"},{"location":"developer_guides/custom_migrations/#optional-fields","title":"Optional fields:","text":"<ul> <li><code>atomic</code>: Set to False to disable atomic migration.</li> <li><code>disable_audit</code>: Set to True to disable model auditing for this migration.</li> </ul> <p>Run with: <pre><code>python manage.py my_migration              # Execute\npython manage.py my_migration --dry-run    # Preview\npython manage.py my_migration --force      # Re-run\n</code></pre></p>"},{"location":"developer_guides/custom_migrations/#django-migration","title":"Django Migration","text":"<p>Use <code>RunDataMigration</code> to run your management command within a Django migration:</p> <pre><code>from django.db import migrations\nfrom apps.data_migrations.utils.migrations import RunDataMigration\n\nclass Migration(migrations.Migration):\n    dependencies = [(\"myapp\", \"0001_initial\")]\n    operations = [\n        RunDataMigration(\"my_migration\"),\n    ]\n</code></pre> <p>This automatically handles idempotency.</p>"},{"location":"developer_guides/custom_migrations/#managing-migrations","title":"Managing Migrations","text":"<p>Use the <code>custom_migrations</code> command to view and manage applied migrations:</p> <pre><code>python manage.py custom_migrations list                 # List all\npython manage.py custom_migrations list --name user     # Filter by name\npython manage.py custom_migrations mark &lt;name&gt;          # Mark as applied\npython manage.py custom_migrations unmark &lt;name&gt;        # Remove record\n</code></pre>"},{"location":"developer_guides/custom_migrations/#naming-convention","title":"Naming Convention","text":"<p>Use descriptive names with dates: <code>{description}_{version}_{YYYY_MM_DD}</code></p> <p>Examples: - <code>migrate_user_data_v1_2024_11_21</code> - <code>backfill_team_settings_2024_12_01</code></p>"},{"location":"developer_guides/custom_migrations/#two-three-phase-deployment-workflow","title":"Two / Three-Phase Deployment Workflow","text":"<p>When adding new fields that require data backfilling, use a two or three-phase deployment to safely migrate data in production:</p>"},{"location":"developer_guides/custom_migrations/#phase-1-add-field-and-initial-migration","title":"Phase 1: Add Field and Initial Migration","text":"<p>Goal: Add the new field and backfill existing data.</p> <ol> <li> <p>Create the data model changes:    <pre><code># models.py\nclass User(models.Model):\n    name = models.CharField(max_length=100)\n    normalized_name = models.CharField(max_length=100, blank=True)  # New field\n</code></pre></p> </li> <li> <p>Create a Django schema migration:    <pre><code>python manage.py makemigrations\n</code></pre></p> </li> <li> <p>Create the data migration command:    <pre><code># management/commands/backfill_normalized_names.py\nfrom apps.data_migrations.management.commands.base import IdempotentCommand\nfrom apps.users.models import User\n\nclass Command(IdempotentCommand):\n    help = \"Backfill normalized names for existing users\"\n    migration_name = \"backfill_normalized_names_2024_12_15\"\n\n    def perform_migration(self, dry_run=False):\n        users = User.objects.filter(normalized_name=\"\")\n\n        if dry_run:\n            self.stdout.write(f\"Would update {users.count()} users\")\n            return\n\n        updated = 0\n        for user in users:\n            user.normalized_name = user.name.lower()\n            user.save()\n            updated += 1\n\n        return updated\n</code></pre></p> </li> <li> <p>Deploy and run:</p> </li> <li>Deploy the PR with model and data migration</li> <li>Run manually in production: <code>python manage.py backfill_normalized_names</code></li> <li>Verify the data was migrated correctly</li> </ol>"},{"location":"developer_guides/custom_migrations/#phase-2-add-django-migration-top-up","title":"Phase 2: Add Django Migration Top-Up","text":"<p>Goal: Automatically migrate any new records created after Phase 1.</p> <ol> <li> <p>Keep the field as optional (no model changes needed):    <pre><code># models.py - unchanged from Phase 1\nclass User(models.Model):\n    name = models.CharField(max_length=100)\n    normalized_name = models.CharField(max_length=100, blank=True)  # Still optional\n</code></pre></p> </li> <li> <p>Create a Django migration with the data migration:    <pre><code># migrations/0XXX_backfill_normalized_names_topup.py\nfrom django.db import migrations\nfrom apps.data_migrations.utils.migrations import RunDataMigration\n\nclass Migration(migrations.Migration):\n    dependencies = [(\"users\", \"0XXX_previous_migration\")]\n\n    operations = [\n        RunDataMigration(\"backfill_normalized_names\", command_options={\"force\": True}),\n    ]\n</code></pre></p> </li> <li> <p>Deploy:</p> </li> <li>The migration runs automatically during deployment</li> <li>The data migration command processes any records created between Phase 1 and Phase 2</li> <li>No constraint changes, so no risk of deploy failures</li> </ol>"},{"location":"developer_guides/custom_migrations/#phase-3-make-field-required-optional","title":"Phase 3: Make Field Required (Optional)","text":"<p>Goal: Optionally enforce the field constraint after all data is migrated.</p> <p>Note: This phase is only needed if you want to make the field required. If the field can remain optional, you can stop after Phase 2.</p> <ol> <li> <p>Update the model to make the field required:    <pre><code># models.py\nclass User(models.Model):\n    name = models.CharField(max_length=100)\n    normalized_name = models.CharField(max_length=100)  # Remove blank=True\n</code></pre></p> </li> <li> <p>Create a schema migration:    <pre><code>python manage.py makemigrations\n</code></pre></p> </li> </ol> <p>This generates:    <pre><code># migrations/0XXX_alter_user_normalized_name.py\nfrom django.db import migrations\n\nclass Migration(migrations.Migration):\n    dependencies = [(\"users\", \"0XXX_previous_migration\")]\n\n    operations = [\n        migrations.AlterField(\n            model_name=\"user\",\n            name=\"normalized_name\",\n            field=models.CharField(max_length=100),  # No longer blank=True\n        ),\n    ]\n</code></pre></p> <ol> <li>Deploy:</li> <li>The constraint is applied to the field</li> <li>All data should already be migrated from Phase 2</li> </ol>"},{"location":"developer_guides/custom_migrations/#why-three-phases","title":"Why Three Phases?","text":"<p>Non-Blocking Deploys: Long-running data migrations can block deployments. Running manually in Phase 1 keeps deploys fast and allows you to monitor progress separately.</p> <p>Deploy Safety: Phase 2 keeps the field optional during the automatic top-up migration. This prevents deployment failures from constraint violations if any unmigrated records exist.</p> <p>Constraint Isolation: Phase 3 (optional) separates the constraint change from the data migration. If you need to make the field required, you can do so safely after confirming all data is migrated. If the field can remain optional, Phase 3 isn't necessary.</p> <p>Performance: Run the initial backfill manually with monitoring. Large datasets can be processed in batches or during low-traffic periods.</p> <p>Flexibility: Test the migration in production with the field as optional. If issues arise in Phase 2, you can fix data before optionally enforcing the constraint in Phase 3.</p> <p>Zero Downtime: Application code continues working with the optional field through Phases 1 and 2. Phase 3 (if needed) only proceeds after verifying all data is migrated.</p>"},{"location":"developer_guides/custom_migrations/#alternative-single-phase-for-simple-cases","title":"Alternative: Single-Phase for Simple Cases","text":"<p>For small datasets or non-critical fields, you can combine all three phases:</p> <pre><code># migrations/0XXX_add_normalized_name.py\nfrom django.db import migrations\nfrom apps.data_migrations.utils.migrations import RunDataMigration\n\nclass Migration(migrations.Migration):\n    dependencies = [(\"users\", \"0XXX_previous_migration\")]\n\n    operations = [\n        migrations.AddField(\n            model_name=\"user\",\n            name=\"normalized_name\",\n            field=models.CharField(max_length=100, blank=True),\n        ),\n        RunDataMigration(\"backfill_normalized_names\"),\n        migrations.AlterField(\n            model_name=\"user\",\n            name=\"normalized_name\",\n            field=models.CharField(max_length=100),  # Now required\n        ),\n    ]\n</code></pre> <p>Use single-phase only when: - Dataset is small (&lt; 10,000 records) - Migration is fast (&lt; 30 seconds) - Field is non-critical - You have tested thoroughly in staging</p>"},{"location":"developer_guides/deleting_models/","title":"Deleting LLM Models","text":"<p>When removing deprecated LLM models from the platform, follow this process to ensure proper cleanup and user notification:</p>"},{"location":"developer_guides/deleting_models/#step-1-update-model-definitions","title":"Step 1: Update Model Definitions","text":"<p>In <code>apps/service_providers/llm_service/default_models.py</code>:</p> <ol> <li> <p>Remove the model from <code>DEFAULT_LLM_PROVIDER_MODELS</code>:    <pre><code>DEFAULT_LLM_PROVIDER_MODELS = {\n    \"openai\": [\n        # Model(\"gpt-4\", k(8)),  # Remove this line\n        Model(\"gpt-4o\", 128000),\n        # ... other models\n    ],\n}\n</code></pre></p> </li> <li> <p>Add the model to <code>DELETED_MODELS</code>:    <pre><code>DELETED_MODELS = [\n    (\"openai\", \"gpt-4\"),\n    (\"anthropic\", \"claude-2.0\"),\n    # ... other deleted models\n]\n</code></pre></p> </li> </ol>"},{"location":"developer_guides/deleting_models/#step-2-create-django-migration","title":"Step 2: Create Django Migration","text":"<p>Create a migration in <code>apps/service_providers/migrations/</code> that performs the model update:</p> <pre><code>from django.db import migrations\n\nfrom apps.data_migrations.utils.migrations import RunDataMigration\nfrom apps.service_providers.migration_utils import llm_model_migration\n\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        (\"service_providers\", \"0040_previous_migration\"),\n    ]\n\n    operations = [\n        # Update model list (marks deprecated models)\n        llm_model_migration(),\n\n        # Clean up references and notify teams\n        RunDataMigration(\"remove_deprecated_models\", command_options={\"force\": True}),\n    ]\n</code></pre>"},{"location":"developer_guides/deleting_models/#testing","title":"Testing","text":"<p>Before deploying:</p> <pre><code># Preview what will be deleted\npython manage.py remove_deprecated_models --dry-run\n\n# Verbose output with team details\npython manage.py remove_deprecated_models --dry-run -v 2\n\n# Run the migration\npython manage.py migrate\n</code></pre>"},{"location":"developer_guides/deleting_models/#maintenance-clearing-old-models-from-deleted_models","title":"Maintenance: Clearing Old Models from DELETED_MODELS","text":"<p>The <code>DELETED_MODELS</code> list should be cleaned up periodically to avoid accumulating stale entries.</p>"},{"location":"developer_guides/deleting_models/#when-to-remove-models","title":"When to Remove Models","text":"<p>It is safe to remove models from <code>DELETED_MODELS</code> when both conditions are met:</p> <ol> <li>The <code>remove_deprecated_models</code> command has run successfully in all environments (development, staging, production)</li> <li>The models have been in the <code>main</code> branch for more than 1 month</li> </ol>"},{"location":"developer_guides/deleting_models/#why-wait-1-month","title":"Why Wait 1 Month?","text":"<ul> <li>Ensures all environments have executed the migration</li> <li>Gives time for any rollback scenarios</li> <li>Allows thorough testing across deployment cycles</li> <li>Accounts for environments that deploy less frequently</li> </ul>"},{"location":"developer_guides/deleting_models/#how-to-clean-up","title":"How to Clean Up","text":"<ol> <li> <p>Verify migration has run everywhere:    <pre><code># Check if migration is applied\npython manage.py custom_migrations list --name remove_deprecated_models\n</code></pre></p> </li> <li> <p>Check git history to confirm models have been in main for 1+ month:    <pre><code># Find when models were added to DELETED_MODELS\ngit log -p --all -S 'DELETED_MODELS' -- apps/service_providers/llm_service/default_models.py\n</code></pre></p> </li> <li> <p>Remove old entries from <code>DELETED_MODELS</code>:    <pre><code>DELETED_MODELS = [\n    # Keep recent additions (&lt; 1 month in main)\n    (\"openai\", \"gpt-4\"),\n    (\"anthropic\", \"claude-2.0\"),\n\n    # Remove these - added 2 months ago, migrated everywhere\n    # (\"azure\", \"gpt-35-turbo\"),\n    # (\"groq\", \"llama3-70b-8192\"),\n]\n</code></pre></p> </li> <li> <p>Commit the cleanup:    <pre><code>git add apps/service_providers/llm_service/default_models.py\ngit commit -m \"chore: clean up DELETED_MODELS list\"\n</code></pre></p> </li> </ol>"},{"location":"developer_guides/deleting_models/#important-notes","title":"Important Notes","text":"<ul> <li>Do not remove models that are still in active migrations</li> <li>Do not remove models if any environment hasn't deployed the migration yet</li> <li>This cleanup is purely for code hygiene; it doesn't affect functionality once migrations have run</li> </ul>"},{"location":"developer_guides/deployment/","title":"Deployment Process","text":""},{"location":"developer_guides/deployment/#continuous-deployment","title":"Continuous Deployment","text":"<ul> <li>All changes merged to the <code>main</code> branch trigger an automated deployment.</li> <li>Deployment is only triggered by successful completion of the lint_and_test.yml GitHub workflow.</li> <li>Deployment is executed by the deploy.yml GitHub action.</li> </ul>"},{"location":"developer_guides/deployment/#monitoring-deployments","title":"Monitoring Deployments","text":"<ul> <li>Deploy notifications are automatically sent to #open-chat-studio-dev Slack channel.</li> <li>Each notification includes:<ul> <li>Build status</li> <li>Changes included in the deployment</li> <li>Deploy completion status</li> </ul> </li> </ul>"},{"location":"developer_guides/deployment/#best-practices","title":"Best Practices","text":"<ul> <li>Always monitor the Slack channels after merging to main.</li> <li>Watch for successful completion of your deployment.</li> <li>Watch for Sentry errors after deployment.</li> </ul>"},{"location":"developer_guides/deployment/#rollback-process","title":"Rollback Process","text":"<ul> <li>Although rollback is possible, we would prefer to roll forward by deploying a fix to the issue.</li> <li>If issues are detected, notify the team in #open-chat-studio-dev.</li> <li>Monitor #ocs-ops for any related errors.</li> <li>Work with the team to determine the best course of action.</li> </ul>"},{"location":"developer_guides/dynamic_filters/","title":"Dynamic Filters","text":"<p>Dynamic filters provide a flexible way to filter data on the frontend and backend. This guide explains how to use and create new dynamic filters.</p>"},{"location":"developer_guides/dynamic_filters/#how-it-works","title":"How it Works","text":"<p>The dynamic filtering system is composed of two main parts: a backend that defines the filtering logic and a frontend that provides the user interface.</p>"},{"location":"developer_guides/dynamic_filters/#backend","title":"Backend","text":"<p>The backend is responsible for defining the filters and applying them to the database queries. The core components are:</p> <ul> <li> <p><code>ColumnFilter</code>: An abstract base class that defines the interface for a single column filter. Each <code>ColumnFilter</code> implementation processes URL parameters and applies the appropriate database filter to a queryset. The base class automatically maps operators to methods (e.g., \"equals\" \u2192 <code>apply_equals</code>).</p> </li> <li> <p><code>MultiColumnFilter</code>: A container class that holds a list of <code>ColumnFilter</code> instances and applies them to a queryset. It provides a <code>columns()</code> class method to list available filter column names and handles the orchestration of applying multiple filters.</p> </li> <li> <p>Filter Types: <code>StringColumnFilter</code>, <code>ChoiceColumnFilter</code>, and <code>TimestampFilter</code> provide pre-built implementations for common filtering patterns, reducing boilerplate code.</p> </li> <li> <p>Filter Implementations: Concrete implementations of <code>ColumnFilter</code> can be found in <code>apps/web/dynamic_filters/column_filters.py</code> (base filters like <code>TimestampFilter</code>, <code>ParticipantFilter</code>) and in app-specific <code>filters.py</code> files throughout the project (e.g., <code>apps/experiments/filters.py</code>).</p> </li> <li> <p>FilterParams &amp; ColumnFilterData: <code>FilterParams</code> extracts and organizes filter data from request query parameters into <code>ColumnFilterData</code> objects, which contain the column name, operator, and value for each filter.</p> </li> </ul>"},{"location":"developer_guides/dynamic_filters/#frontend","title":"Frontend","text":"<p>The frontend is built using Alpine.js and HTMX. It dynamically generates the filter UI based on the configuration provided by the backend.</p> <ul> <li>Filter Template: The main filter template is <code>templates/experiments/filters.html</code>. This template contains the Alpine.js component that manages the filter state and UI.</li> <li>Filter Configuration: The backend provides the filter configuration to the frontend through a set of <code>df_*</code> context variables. These variables are passed as JSON scripts in the HTML and then used to initialize the Alpine.js component. See <code>apps/experiments/views/experiment.py</code> for an example of how this data is provided to the template.</li> </ul>"},{"location":"developer_guides/dynamic_filters/#linking-query-parameters-to-orm-operations","title":"Linking query parameters to ORM operations","text":""},{"location":"developer_guides/dynamic_filters/#query-parameters","title":"Query Parameters","text":"<p>Filter values are passed through URL query parameters using a structured naming convention:</p> <ul> <li><code>filter_{i}_column</code> - Specifies which column to filter on (matches the <code>query_param</code> of a <code>ColumnFilter</code>)</li> <li><code>filter_{i}_operator</code> - Defines the filter operation (e.g., equals, contains, before, after)</li> <li><code>filter_{i}_value</code> - Contains the actual filter value</li> </ul> <p>The <code>{i}</code> represents the filter index (0 to <code>MAX_FILTER_PARAMS-1</code>), allowing multiple filters to be applied simultaneously (e.g., <code>filter_0_column</code>, <code>filter_1_column</code>, etc.).</p>"},{"location":"developer_guides/dynamic_filters/#filterparams-and-columnfilterdata","title":"FilterParams and ColumnFilterData","text":"<p>The <code>FilterParams</code> class extracts filter parameters from request query parameters and organizes them into <code>ColumnFilterData</code> objects. Each <code>ColumnFilterData</code> contains the column name, operator, and value for a single filter.</p>"},{"location":"developer_guides/dynamic_filters/#column-filter","title":"Column Filter","text":"<p>The <code>ColumnFilter</code> class acts as a bridge between query parameters and ORM filters. Each filter defines a <code>query_param</code> attribute that corresponds to the column name in the query parameters. When a request contains <code>filter_{i}_column</code> matching this <code>query_param</code>, the filter processes the associated operator and value to generate the appropriate database query.</p> <p>The <code>ColumnFilter.apply()</code> method: 1. Retrieves the <code>ColumnFilterData</code> for its <code>query_param</code> from <code>FilterParams</code> 2. Converts the operator to a method name (e.g., \"starts with\" \u2192 <code>apply_starts_with</code>) 3. Calls the appropriate <code>apply_*</code> method with the parsed value</p>"},{"location":"developer_guides/dynamic_filters/#available-filter-types","title":"Available Filter Types","text":"<p>The dynamic filter system provides several filter types that implements common filtering patterns:</p>"},{"location":"developer_guides/dynamic_filters/#stringcolumnfilter","title":"StringColumnFilter","text":"<p>Provides methods for string-based filtering operations: - <code>apply_equals()</code> - Exact match - <code>apply_contains()</code> - Case-insensitive contains - <code>apply_does_not_contain()</code> - Case-insensitive exclusion - <code>apply_starts_with()</code> - Case-insensitive starts with - <code>apply_ends_with()</code> - Case-insensitive ends with - <code>apply_any_of()</code> - Match any value from a JSON list</p> <p>Requires setting a <code>columns</code> class variable with a list of database field paths. When multiple columns are provided, the filter automatically applies OR logic across all specified fields.</p> <p>Examples: <pre><code># Single column search\nStringColumnFilter(columns=[\"name\"], query_param=\"name\", label=\"Name\")\n\n# Multiple columns with OR logic (searches identifier OR name)\nStringColumnFilter(columns=[\"identifier\", \"name\"], query_param=\"participant\", label=\"Participant\")\n</code></pre></p>"},{"location":"developer_guides/dynamic_filters/#choicecolumnfilter","title":"ChoiceColumnFilter","text":"<p>Provides methods for choice-based filtering operations: - <code>apply_any_of()</code> - Match any value from a list - <code>apply_all_of()</code> - Match all values from a list (AND logic) - <code>apply_excludes()</code> - Exclude all values from a list</p> <p>Requires setting a <code>column</code> class variable with the database field path.</p>"},{"location":"developer_guides/dynamic_filters/#available-operators","title":"Available Operators","text":"<p>The system supports the following operators defined in the <code>Operators</code> enum:</p> <ul> <li>String operations: <code>equals</code>, <code>contains</code>, <code>does not contain</code>, <code>starts with</code>, <code>ends with</code>, <code>any of</code></li> <li>Date/time operations: <code>on</code>, <code>before</code>, <code>after</code>, <code>range</code></li> <li>Choice operations: <code>any of</code>, <code>all of</code>, <code>excludes</code></li> </ul> <p>Operators are configured in the <code>ColumnFilter</code> class. They are determined automatically based on the filter type but can be overridden.</p>"},{"location":"developer_guides/dynamic_filters/#step-by-step-walkthrough-creating-a-product-inventory-filter","title":"Step-by-Step Walkthrough: Creating a Product Inventory Filter","text":"<p>This walkthrough will guide you through creating a complete filtering system for a hypothetical product inventory feature. We'll create a custom filter column, integrate it into a multi-column filter, and wire it up to a view.</p>"},{"location":"developer_guides/dynamic_filters/#step-1-create-a-custom-column-filter","title":"Step 1: Create a Custom Column Filter","text":"<p>First, let's create a filter for product categories using the existing filter types:</p> <pre><code># apps/inventory/filters.py\nfrom apps.web.dynamic_filters.base import ChoiceColumnFilter, StringColumnFilter\nfrom apps.web.dynamic_filters.column_filters import TimestampFilter\n\nclass ProductCategoryFilter(ChoiceColumnFilter):\n    \"\"\"Filter products by category name.\"\"\"\n    query_param: str = \"category\"\n    column: str = \"category__name\"  # Database field path\n    label: str = \"Category\"\n\n    def prepare(self, team, **kwargs):\n        self.options = [\n            {\"id\": cat.id, \"label\": cat.name}\n            for cat in Category.objects.filter(team=team).all()\n        ]\n\np_filter = ProductCategoryFilter()\n\n# Alternately, you can construct it directly using kwargs:\n\np_filter = ChoiceColumnFilter(label=\"Category\", query_param=\"category\", column=\"category__name\", options=[...])\n</code></pre>"},{"location":"developer_guides/dynamic_filters/#step-2-create-a-multi-column-filter","title":"Step 2: Create a Multi-Column Filter","text":"<p>Now let's create a multi-column filter that combines our new category filter with existing filters:</p> <pre><code># apps/inventory/filters.py (continued)\nfrom typing import ClassVar\nfrom collections.abc import Sequence\nfrom apps.web.dynamic_filters.base import MultiColumnFilter\n\nclass ProductInventoryFilter(MultiColumnFilter):\n    \"\"\"Filter for product inventory using multiple column filters.\"\"\"\n\n    filters: ClassVar[Sequence[ColumnFilter]] = [\n        ProductCategoryFilter(),\n        TimestampFilter(label=\"Created At\", column=\"created_at\", query_param=\"created_date\"),\n        TimestampFilter(label=\"Updated At\", column=\"updated_at\", query_param=\"last_updated\"),\n        # Example: Search category name or description with OR logic\n        StringColumnFilter(\n            label=\"Category\",\n            columns=[\"category__name\", \"category__description\"],  # Searches both fields with OR logic\n            query_param=\"category_search\"\n        ),\n    ]\n\n    def prepare_queryset(self, queryset):\n        \"\"\"Prepare the queryset with any necessary annotations or select_related calls.\"\"\"\n        return queryset.select_related('category')\n</code></pre>"},{"location":"developer_guides/dynamic_filters/#step-3-create-the-model-and-table-for-completeness","title":"Step 3: Create the Model and Table (for completeness)","text":"<pre><code># apps/inventory/models.py\nfrom django.db import models\n\nclass Category(models.Model):\n    name = models.CharField(max_length=100)\n    description = models.TextField(blank=True)\n\nclass Product(models.Model):\n    name = models.CharField(max_length=200)\n    category = models.ForeignKey(Category, on_delete=models.CASCADE)\n    price = models.DecimalField(max_digits=10, decimal_places=2)\n    stock_quantity = models.IntegerField()\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n</code></pre> <pre><code># apps/inventory/tables.py\nimport django_tables2 as tables\nfrom .models import Product\n\nclass ProductTable(tables.Table):\n    class Meta:\n        model = Product\n        fields = ('name', 'category', 'price', 'stock_quantity', 'created_at', 'updated_at')\n        attrs = {'class': 'table table-striped'}\n</code></pre>"},{"location":"developer_guides/dynamic_filters/#step-4-create-the-view","title":"Step 4: Create the View","text":"<p>Create a view that uses our new filter system:</p> <pre><code># apps/inventory/views.py\nfrom django.views.generic import TemplateView\nfrom django_tables2 import SingleTableView\nfrom apps.web.dynamic_filters.datastructures import FilterParams\nfrom apps.experiments.filters import get_filter_context_data\nfrom .models import Product\nfrom .tables import ProductTable\nfrom .filters import ProductInventoryFilter\n\nclass ProductInventoryView(SingleTableView):\n    \"\"\"View for displaying filtered product inventory.\"\"\"\n    model = Product\n    table_class = ProductTable\n    template_name = \"inventory/product_list.html\"\n\n    def get_queryset(self):\n        \"\"\"Apply filters to the queryset.\"\"\"\n        queryset = super().get_queryset()\n\n        # Create filter instance and apply it\n        product_filter = ProductInventoryFilter()\n        timezone = self.request.session.get(\"detected_tz\")\n\n        filter_params = FilterParams.from_request(self.request)\n        return product_filter.apply(queryset, filter_params, timezone)\n\n    def get_context_data(self, **kwargs):\n        \"\"\"Add filter configuration to the template context.\"\"\"\n        context = super().get_context_data(**kwargs)\n\n        # Add filter context data using the helper function\n        filter_context = get_filter_context_data(\n            team=self.request.team,  # Assuming team is available in request\n            columns=ProductInventoryFilter.columns(self.request.team),\n            filter_class=ProductInventoryFilter,\n            table_url=reverse(\"inventory:product_table\"),  # Your HTMX table URL\n            table_container_id=\"product-table\",\n            table_type=\"your-table-type\"\n        )\n\n        context.update(filter_context)\n        return context\n</code></pre>"},{"location":"developer_guides/dynamic_filters/#step-5-create-the-template-and-update-filters","title":"Step 5: Create the Template and Update Filters","text":"<p>Create the template that includes the filter interface:</p> <pre><code>&lt;!-- apps/inventory/templates/inventory/product_list.html --&gt;\n{% extends \"base.html\" %}\n{% load django_tables2 %}\n\n{% block title %}Product Inventory{% endblock title %}\n\n{% block content %}\n&lt;div class=\"container mx-auto px-4 py-8\"&gt;\n    &lt;h1 class=\"text-2xl font-bold mb-6\"&gt;Product Inventory&lt;/h1&gt;\n    &lt;!-- Include the dynamic filters --&gt;\n    {% include \"experiments/filters.html\" with df_table_type=\"your-table-type\" %}\n    &lt;!-- Rest of the page --&gt;\n&lt;/div&gt;\n\n{% endblock content %}\n</code></pre>"},{"location":"developer_guides/feature_flags/","title":"Feature Flags","text":"<p>Open Chat Studio uses Django Waffle for feature flags with a custom team-based implementation. Feature flags allow you to toggle features on/off for specific teams without code deployments.</p>"},{"location":"developer_guides/feature_flags/#overview","title":"Overview","text":"<p>Feature flags in Open Chat Studio are: - Team-scoped: Flags can be enabled/disabled per team - Database-driven: Stored in the database and configurable via a custom admin page - Cached: Uses Redis for performance - Convention-based: New flags must follow naming conventions and be registered in <code>apps/teams/flags.py</code></p>"},{"location":"developer_guides/feature_flags/#custom-flag-model","title":"Custom Flag Model","text":"<p>The system uses a custom <code>Flag</code> model (<code>apps.teams.models.Flag</code>) that extends Django Waffle's <code>AbstractUserFlag</code> to support team-based activation:</p> <pre><code>from apps.teams.models import Flag\n\n# Create a flag\nflag = Flag.objects.create(name=\"flag_new_feature\")\n\n# Activate for specific teams\nflag.teams.add(my_team)\n\n# Check if active for a team\nflag.is_active_for_team(my_team)\n</code></pre> <p>Tip</p> <p>Flags are created automatically in the database when referenced in code or templates so it is not necessary to create them manually.</p>"},{"location":"developer_guides/feature_flags/#registering-a-flag","title":"Registering a Flag","text":"<p>All flags must be declared in <code>apps/teams/flags.py</code> as entries in the <code>Flags</code> enum before use:</p> <pre><code>from apps.teams.flags import Flags\n\nclass Flags(FlagInfo, Enum):\n    MY_FEATURE = (\n        \"flag_my_feature\",   # slug \u2014 must start with \"flag_\"\n        \"Short description\", # shown in admin and team settings UI\n        \"docs-slug\",         # key into settings.DOCUMENTATION_LINKS (use \"\" if none)\n        [],                  # other flag slugs this flag requires (auto-enabled together)\n        True,                # teams_can_manage: whether team admins can toggle this themselves\n    )\n</code></pre> <p>This registry drives the team settings UI (<code>teams_can_manage=True</code> flags appear there) and the <code>check_flag_usage</code> audit command. Flags are created in the database automatically on first use \u2014 no migration needed.</p> <p>When referencing a flag in Python code, prefer the enum to avoid string typos:</p> <pre><code>from apps.teams.flags import Flags\nfrom waffle import flag_is_active\n\nif flag_is_active(request, Flags.MY_FEATURE.slug):\n    ...\n</code></pre>"},{"location":"developer_guides/feature_flags/#naming-convention","title":"Naming Convention","text":"<p>All new feature flags MUST be prefixed with <code>flag_</code></p> <pre><code># \u2705 Correct\n\"flag_new_dashboard\"\n\"flag_enhanced_chat\"\n\n# \u274c Incorrect - will raise ValidationError\n\"new_dashboard\"\n</code></pre> <p>This naming convention:</p> <ul> <li>Prevents naming conflicts</li> <li>Makes flags easily identifiable in code</li> <li>Enables better tooling and management</li> </ul>"},{"location":"developer_guides/feature_flags/#usage-in-code","title":"Usage in Code","text":""},{"location":"developer_guides/feature_flags/#python","title":"Python","text":"<p>In Django views or other Python code where a request object is available, you can check if a feature flag is active for the current team or user:</p> <pre><code>from waffle import flag_is_active\n\ndef my_view(request):\n    if flag_is_active(request, \"flag_new_feature\"):\n        # New feature code\n        return render(request, \"new_template.html\")\n    else:\n        # Legacy code\n        return render(request, \"old_template.html\")\n</code></pre> <p>When a request object is not available, you can still check if a flag is active by using the <code>Flag</code> model directly:</p> <pre><code>flag = Flag.get(\"flag_new_feature\")\nflag.is_active_for_team(team)\nflag.is_active_for_user(user)\n</code></pre>"},{"location":"developer_guides/feature_flags/#django-templates","title":"Django Templates","text":"<pre><code>{% load waffle_tags %}\n\n{% flag \"flag_new_feature\" %}\n    &lt;div class=\"new-feature\"&gt;\n        &lt;!-- New feature UI --&gt;\n    &lt;/div&gt;\n{% endflag %}\n</code></pre>"},{"location":"developer_guides/feature_flags/#pipeline-node-fields","title":"Pipeline node fields","text":"<p>For pipeline node fields that should only be visible in the UI when a flag is active, use <code>flag_required</code>:</p> <pre><code>my_field: str = Field(..., flag_required=\"flag_my_feature\")\n</code></pre>"},{"location":"developer_guides/feature_flags/#removing-a-flag-rolling-out-to-everyone","title":"Removing a Flag (Rolling Out to Everyone)","text":"<p>When a feature is stable and should be on for all users, remove the flag entirely rather than leaving dead code:</p> <ol> <li>Run <code>check_flag_usage</code> to find every reference.</li> <li>Remove all <code>flag_is_active</code> / <code>{% flag %}</code> / <code>override_flag</code> guards, keeping the guarded code.</li> <li>Delete the <code>Flags</code> enum entry from <code>apps/teams/flags.py</code>.</li> <li>Update or remove tests that used <code>override_flag</code> for this flag \u2014 test the behaviour unconditionally.</li> <li>The flag row in the database becomes orphaned and can be deleted via the Django admin.</li> </ol>"},{"location":"developer_guides/feature_flags/#management-commands","title":"Management Commands","text":""},{"location":"developer_guides/feature_flags/#check-flag-usage","title":"Check Flag Usage","text":"<p>Use the <code>check_flag_usage</code> management command to find where flags are used in the codebase:</p> <pre><code># Check all flags\npython manage.py check_flag_usage\n\n# Check specific flag\npython manage.py check_flag_usage --flag-name flag_new_feature\n</code></pre> <p>Output example: <pre><code>Found 5 flags in database\n\nFlags found in code (3):\n  \u2713 flag_new_dashboard\n    - apps/web/views.py\n    - templates/dashboard.html\n  \u2713 flag_enhanced_chat\n    - apps/chat/views.py\n\nFlags not found in code (2):\n  \u2717 flag_old_feature\n  \u2717 flag_experimental_ui\n</code></pre></p> <p>This helps identify:</p> <ul> <li>Active flags: Currently used in code</li> <li>Dead flags: No longer referenced and can be removed</li> </ul>"},{"location":"developer_guides/feature_flags/#best-practices","title":"Best Practices","text":""},{"location":"developer_guides/feature_flags/#naming","title":"Naming","text":"<ul> <li>Use descriptive names: <code>flag_enhanced_search</code> not <code>flag_search</code></li> <li>Include the feature area: <code>flag_chat_reactions</code>, <code>flag_dashboard_v2</code></li> <li>Avoid abbreviations: <code>flag_new_authentication</code> not <code>flag_new_auth</code></li> </ul>"},{"location":"developer_guides/feature_flags/#code-organization","title":"Code Organization","text":"<ul> <li>Keep flag logic simple and readable</li> <li>Avoid deep nesting of feature flag conditions</li> <li>Consider extracting flag-dependent code into separate functions/classes</li> </ul> <pre><code># \u2705 Good\ndef get_dashboard_data(request):\n    if flag_is_active(request, 'flag_new_dashboard'):\n        return get_enhanced_dashboard_data(request)\n    return get_legacy_dashboard_data(request)\n\n# \u274c Avoid deep nesting\ndef complex_view(request):\n    if flag_is_active(request, 'flag_feature_a'):\n        if flag_is_active(request, 'flag_feature_b'):\n            # Deep nesting makes code hard to follow\n</code></pre>"},{"location":"developer_guides/feature_flags/#documentation","title":"Documentation","text":"<ul> <li>Document the purpose of each flag</li> <li>Include rollout plans in PR descriptions</li> <li>Update team documentation when adding user-facing features</li> </ul>"},{"location":"developer_guides/feature_flags/#testing","title":"Testing","text":"<ul> <li>Test both flag states (enabled/disabled)</li> <li>Include flag state in test names</li> </ul> <pre><code>def test_dashboard_with_new_feature_enabled(self):\n    with override_flag('flag_new_dashboard', active=True):\n        # Test new behavior\n\ndef test_dashboard_with_new_feature_disabled(self):\n    with override_flag('flag_new_dashboard', active=False):\n        # Test legacy behavior\n</code></pre>"},{"location":"developer_guides/igor/","title":"Claude Workflows","text":"<p>GitHub Actions workflows for automated issue implementation, incremental task progress, and CI followup.</p>"},{"location":"developer_guides/igor/#use-cases","title":"Use Cases","text":"<ul> <li>Implement an issue end-to-end when labeled or mentioned</li> <li>Work through multi-task issues incrementally (one task per run)</li> <li>Automatically fix CI failures and address review comments on Claude PRs</li> </ul>"},{"location":"developer_guides/igor/#how-to-use","title":"How to Use","text":""},{"location":"developer_guides/igor/#one-off-label-an-issue","title":"One-off: label an issue","text":"<ol> <li>Apply the <code>claude</code> label to an issue</li> <li>Claude creates a branch, implements the work, and opens a PR</li> </ol>"},{"location":"developer_guides/igor/#incremental-multi-task-issues","title":"Incremental: multi-task issues","text":"<ol> <li>Create a tracking issue with the <code>claude</code> label using the format below</li> <li>The workflow runs daily at 2am UTC and picks the oldest eligible issue</li> <li>Each run implements one unchecked task and creates a PR</li> <li>After the PR merges, the next run picks up the next task</li> <li>Can also be triggered manually via Actions &gt; Claude Code &gt; Run workflow</li> </ol>"},{"location":"developer_guides/igor/#interactive-claude-in-comments","title":"Interactive: @claude in comments","text":"<p>Mention <code>@claude</code> in any issue or PR comment to get a response or request changes.</p>"},{"location":"developer_guides/igor/#issue-format-for-incremental-tasks","title":"Issue Format (for incremental tasks)","text":"<pre><code>## Goal\nBrief description of what the project aims to achieve.\n\n## Context\nOptional background info the AI should know about.\n\n## Tasks\n\n### Task 1: Short description\n- [ ] Task 1\n\nDetailed context for this task. Include relevant file paths, expected\nbehavior, edge cases, or links to related code.\n\n### Task 2: Short description\n- [ ] Task 2\n\nMore context here. The more specific you are about what \"done\" looks\nlike, the better the result.\n\n### Task 3: Already completed\n- [x] Task 3\n\n### Task 4: Blocked task\n- [ ] blocked: Task 4 - explain why\n\n## Learnings\n&lt;!-- AI updates this section with discoveries --&gt;\n</code></pre> <p>Each task gets its own section with a checkbox and a context block. The checkbox is what the workflow uses to track progress \u2014 keep it on its own line.</p>"},{"location":"developer_guides/igor/#automatic-follow-up","title":"Automatic Follow-up","text":"<p>After Claude creates a PR, the Claude Followup workflow automatically runs one round of fixes when CI completes:</p> <ol> <li>Waits for the \"Lint and Test\" workflow to complete on any <code>claude/**</code> branch</li> <li>Checks for the <code>claude-followup-done</code> label \u2014 if present, skips (one-round limit)</li> <li>Reads CI failure logs and review comments</li> <li>Fixes lint, type, test, and lockfile issues</li> <li>Addresses actionable review feedback</li> <li>Pushes fixes and comments on the PR with a summary</li> <li>Adds the <code>claude-followup-done</code> label to prevent re-runs</li> </ol>"},{"location":"developer_guides/igor/#manual-trigger","title":"Manual Trigger","text":"<p>Run on a specific issue via Actions &gt; Claude Code &gt; Run workflow, then enter the issue number.</p>"},{"location":"developer_guides/igor/#files","title":"Files","text":"<ul> <li><code>.github/workflows/claude.yml</code> \u2014 Main workflow (event-driven + scheduled)</li> <li><code>.github/workflows/claude-followup.yml</code> \u2014 Automatic CI followup</li> </ul>"},{"location":"developer_guides/index_managers/","title":"Index Manager Classes","text":"<p>Index managers provide an abstraction layer for managing document embeddings and vector stores in Open Chat Studio. They are LLM provider-specific implementations that enable both remote (provider-hosted) and local (self-hosted) indexing strategies for document collections.</p>"},{"location":"developer_guides/index_managers/#architecture-overview","title":"Architecture Overview","text":"<p>The index manager system follows an abstract base class pattern with two main hierarchies:</p> <pre><code>RemoteIndexManager (ABC)\n\u251c\u2500\u2500 OpenAIRemoteIndexManager\n\nLocalIndexManager (ABC)\n\u251c\u2500\u2500 OpenAILocalIndexManager\n</code></pre> <p>The system supports two indexing strategies: - Remote indexing: Vector stores are created and managed by external providers (e.g., OpenAI) - Local indexing: Embeddings are generated locally and stored in the application database</p>"},{"location":"developer_guides/index_managers/#core-classes","title":"Core Classes","text":""},{"location":"developer_guides/index_managers/#remoteindexmanager","title":"RemoteIndexManager","text":"<p>Abstract base class for managing vector stores in remote indexing services. Provides a common interface for interacting with external vector store providers.</p>"},{"location":"developer_guides/index_managers/#openairemoteindexmanager","title":"OpenAIRemoteIndexManager","text":"<p>OpenAI-specific implementation for managing vector stores using OpenAI's vector store API.</p>"},{"location":"developer_guides/index_managers/#localindexmanager","title":"LocalIndexManager","text":"<p>Abstract base class for managing local embedding operations. Handles text processing and embedding generation on the application side.</p>"},{"location":"developer_guides/index_managers/#usage-example","title":"Usage Example","text":""},{"location":"developer_guides/index_managers/#getting-an-index-manager","title":"Getting an Index Manager","text":"<p>Index managers are obtained through the Collection model's <code>get_index_manager()</code> method:</p> <pre><code>from apps.documents.models import Collection\n\n# Get collection\ncollection = Collection.objects.get(id=collection_id)\n\n# Get appropriate index manager based on collection configuration\nindex_manager = collection.get_index_manager()\n</code></pre> <p>The method returns: - <code>RemoteIndexManager</code> instance if <code>collection.is_remote_index</code> is <code>True</code> - <code>LocalIndexManager</code> instance if <code>collection.is_remote_index</code> is <code>False</code></p>"},{"location":"developer_guides/index_managers/#remote-index-operations","title":"Remote Index Operations","text":"<pre><code># Create a new vector store\ncollection.ensure_remote_index_created(\n    file_ids=[\"file-123\", \"file-456\"]  # Optional initial files.\n)\n\n# Upload file to remote service\nfile = File.objects.get(id=file_id)\nindex_manager.upload_file_to_remote(file)\n\n# Link files to existing vector store with chunking\nindex_manager.link_files_to_remote_index(\n    file_ids=[\"file-789\", \"file-101\"],\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Check if file exists remotely\nexists = index_manager.file_exists_at_remote(file)\n\n# Clean up - delete vector store\nindex_manager.delete_remote_index()\n</code></pre>"},{"location":"developer_guides/index_managers/#local-index-operations","title":"Local Index Operations","text":"<pre><code># Generate embedding for text content\ncontent = \"This is a sample document content.\"\nembedding_vector = index_manager.get_embedding_vector(content)\n\n# Chunk large text with overlap\ntext = \"Long document content here...\"\nchunks = index_manager.chunk_content(\n    text=text,\n    chunk_size=500,\n    chunk_overlap=50\n)\n\n# Process each chunk for storage\nfor i, chunk in enumerate(chunks):\n    embedding = index_manager.get_embedding_vector(chunk)\n    # Store in FileChunkEmbedding model...\n</code></pre>"},{"location":"developer_guides/index_managers/#collection-integration","title":"Collection Integration","text":"<p>Index managers are typically used through the Collection model's indexing methods:</p> <pre><code>from apps.documents.models import Collection, CollectionFile\n\ncollection = Collection.objects.get(id=collection_id)\n\n# Add files to index (automatically chooses remote vs local)\ncollection_files = CollectionFile.objects.filter(\n    collection=collection,\n    status=FileStatus.PENDING\n).iterator(100)\n\ncollection.add_files_to_index(\n    collection_files=collection_files,\n    chunk_size=1000,\n    chunk_overlap=200\n)\n</code></pre>"},{"location":"developer_guides/index_managers/#configuration-driven-selection","title":"Configuration-Driven Selection","text":"<p>The system automatically selects the appropriate manager based on collection settings:</p> <pre><code>def get_index_manager(self):\n    if self.is_index and self.is_remote_index:\n        return self.llm_provider.get_remote_index_manager(\n            self.openai_vector_store_id\n        )\n    else:\n        return self.llm_provider.get_local_index_manager(\n            embedding_model_name=self.embedding_provider_model.name\n        )\n</code></pre>"},{"location":"developer_guides/index_managers/#citations","title":"Citations","text":"<p>The platform has a built-in mechanism for citing sources used by the LLM, particularly when retrieving information from indexed documents. This process involves generation, parsing, and final rendering of citations.</p>"},{"location":"developer_guides/index_managers/#1-citation-generation","title":"1. Citation Generation","text":"<p>When an agent uses a tool like <code>SearchIndexTool</code> to query a document collection, it gains access to the content of relevant files. The LLM is instructed to cite its sources by embedding a special tag in its response whenever it uses information from a file.</p> <p>The citation format is <code>&lt;CIT file-id /&gt;</code>, where <code>file-id</code> is the unique identifier of the cited <code>File</code> model instance. This format is defined by the <code>OCS_CITATION_PATTERN</code> constant found in <code>apps.chat.agent.tools</code>.</p>"},{"location":"developer_guides/index_managers/#2-building-the-reference-section","title":"2. Building the Reference Section","text":"<p>After the LLM generates a response containing these citation tags, the system processes the message to create a human-readable reference section. This is handled by the <code>apps.service_providers.llm_service.utils.populate_reference_section_from_citations</code>).</p> <p>This method performs three main actions: 1.  It scans the AI message for all instances of the <code>&lt;CIT file-id /&gt;</code> pattern. 2.  Each tag is replaced with a footnote-style reference (e.g., <code>[^1]</code>, <code>[^2]</code>). It keeps track of cited files to reuse reference numbers for multiple citations of the same file. 3.  It appends a markdown-formatted reference list to the end of the message. Each entry includes the reference number, the original file name, and a download link for that file.</p> <p>This processing step occurs within the <code>invoke</code> method of the <code>LLMChat</code> runnable (<code>apps/service_providers/llm_service/runnables.py</code>) before the final message is saved and sent to the user.</p> <p>Example transformation:</p> <p>Input from LLM: <pre><code>The sky is blue &lt;CIT 123 /&gt;. The grass is green &lt;CIT 456 /&gt;.\n</code></pre></p> <p>Output after processing: <pre><code>The sky is blue [^1]. The grass is green [^2].\n\n[^1]: [document_a.pdf](https://example.com/download/123)\n[^2]: [source_b.txt](https://example.com/download/456)\n</code></pre></p>"},{"location":"developer_guides/index_managers/#3-final-rendering-in-channels","title":"3. Final Rendering in Channels","text":"<p>The markdown-formatted message, now including footnote-style references, is passed to the channel layer for final display to the user. In <code>apps/chat/channels.py</code>, a function named <code>_format_reference_section</code> is responsible for parsing this markdown and rendering it appropriately for the specific channel (e.g., converting it to HTML for the web interface). This ensures that users see a clean, clickable list of citations in the final UI.</p>"},{"location":"developer_guides/integration_testing/","title":"Integration Testing","text":"<p>Integration tests make real API calls to external services to verify the complete implementation. All integration tests share the same <code>.env.integration</code> configuration file.</p>"},{"location":"developer_guides/integration_testing/#setup","title":"Setup","text":""},{"location":"developer_guides/integration_testing/#create-integration-environment-file","title":"Create Integration Environment File","text":"<p>Copy the example file and add your real API credentials:</p> <pre><code>cp .env.integration.example .env.integration\n</code></pre> <p>Edit <code>.env.integration</code> and add credentials for the services you want to test. See <code>.env.integration.example</code> for all available options and links to get API keys.</p> <p>Note: <code>.env.integration</code> is in <code>.gitignore</code> and will not be committed.</p>"},{"location":"developer_guides/integration_testing/#running-tests","title":"Running Tests","text":""},{"location":"developer_guides/integration_testing/#run-all-integration-tests","title":"Run All Integration Tests","text":"<pre><code>pytest -m integration -v -s\n</code></pre>"},{"location":"developer_guides/integration_testing/#skip-integration-tests-default","title":"Skip Integration Tests (Default)","text":"<pre><code># Run all tests EXCEPT integration tests\npytest -m \"not integration\"\n</code></pre>"},{"location":"developer_guides/integration_testing/#speech-service-integration-tests","title":"Speech Service Integration Tests","text":"<p>Test file: <code>apps/service_providers/tests/test_speech_integration.py</code></p>"},{"location":"developer_guides/integration_testing/#what-the-tests-cover","title":"What the Tests Cover","text":"<ul> <li>OpenAI: Text-to-speech (synthesis) and speech-to-text (transcription)</li> <li>AWS Polly: Text-to-speech synthesis</li> <li>Azure Cognitive Services: Text-to-speech and speech-to-text</li> </ul>"},{"location":"developer_guides/integration_testing/#running-speech-tests","title":"Running Speech Tests","text":"<pre><code>pytest apps/service_providers/tests/test_speech_integration.py -m integration -v -s\n</code></pre>"},{"location":"developer_guides/integration_testing/#management-command-for-quick-testing","title":"Management Command for Quick Testing","text":"<p>Useful for manual testing during development:</p> <pre><code># Test all services\npython manage.py test_speech_live --service all\n\n# Test specific service\npython manage.py test_speech_live --service openai\npython manage.py test_speech_live --service aws\npython manage.py test_speech_live --service azure\n\n# Custom text\npython manage.py test_speech_live --service openai --text \"Hello world\"\n\n# Save audio files\npython manage.py test_speech_live --service all --save-audio /tmp/test_audio\n</code></pre>"},{"location":"developer_guides/integration_testing/#llm-provider-integration-tests","title":"LLM Provider Integration Tests","text":"<p>Test file: <code>apps/service_providers/tests/test_llm_integration.py</code></p>"},{"location":"developer_guides/integration_testing/#running-llm-tests","title":"Running LLM Tests","text":"<pre><code>pytest apps/service_providers/tests/test_llm_integration.py -m integration -v -s\n</code></pre>"},{"location":"developer_guides/integration_testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer_guides/integration_testing/#tests-skip-with-credentials-not-set","title":"Tests Skip with \"Credentials not set\"","text":"<p>Tests automatically skip if required credentials are not found in <code>.env.integration</code>. Add the missing credentials to enable those tests.</p>"},{"location":"developer_guides/integration_testing/#audio-processing-errors-speech-tests","title":"Audio Processing Errors (Speech Tests)","text":"<p>If you see pydub errors, ensure ffmpeg is installed:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install ffmpeg\n\n# macOS\nbrew install ffmpeg\n</code></pre>"},{"location":"developer_guides/notifications/","title":"OCS Notifications App","text":""},{"location":"developer_guides/notifications/#overview","title":"Overview","text":"<p>The <code>ocs_notifications</code> app provides a team-scoped notification system for Open Chat Studio. It alerts team members about important events through in-app messages and email, allowing users to control delivery preferences and severity thresholds.</p> <p>This system is designed to:</p> <ul> <li>Alert team members about important events (custom action health checks, evaluations, data syncs, etc.)</li> <li>Handle per-user bookkeeping automatically (read/unread, mute, do-not-disturb)</li> <li>Support multi-channel delivery with granular user preferences (in-app, email, level thresholds)</li> <li>Reduce notification fatigue through deduplication, muting, and severity filtering</li> <li>Persist notifications so users can view, filter, and manage them later</li> </ul>"},{"location":"developer_guides/notifications/#core-concepts","title":"Core Concepts","text":""},{"location":"developer_guides/notifications/#notification-levels","title":"Notification Levels","text":"<p>Notifications use a three-tier severity system:</p> Level Purpose Example INFO General information, non-critical updates \"Data sync completed successfully\" WARNING Concerning but non-breaking changes \"Custom action response time is slow\" ERROR Critical issues requiring attention \"Custom action is unreachable\" <p>Users configure separate thresholds for in-app and email delivery. For example, a user might see INFO and WARNING in-app but only receive ERROR emails.</p>"},{"location":"developer_guides/notifications/#the-identifier-system","title":"The Identifier System","text":"<p>Notifications are threaded by combining a slug and event data. The app hashes <code>{\"slug\": slug, \"data\": event_data}</code> into a SHA1 identifier, stored on EventType - which identifies an event in the system. When you call <code>create_notification()</code> with the same slug and event_data, the system reuses the same EventType and appends a new NotificationEvent to that thread.</p> <p>How deduplication works:</p> <ul> <li>Same slug + Same event_data \u2192 Reuses the same event thread; creates a new NotificationEvent and re-notifies users who previously read it</li> <li>Same slug + Different event_data \u2192 Creates a separate event thread</li> </ul> <p>Example:</p> <pre><code># First time: Creates event thread + first event\ncreate_notification(\n    slug=\"payment-api-timeout\",\n    event_data={\"api_id\": 1},  # Unique to this API\n    # ... other fields\n)\n\n# Later: Same API times out again\ncreate_notification(\n    slug=\"payment-api-timeout\",\n    event_data={\"api_id\": 1},  # SAME \u2192 same thread\n    # ... other fields\n)\n# \u2192 Same EventType (thread)\n# \u2192 New NotificationEvent created\n# \u2192 Users who read it are marked unread (gets re-notified)\n\n# Different API times out\ncreate_notification(\n    slug=\"payment-api-timeout\",\n    event_data={\"api_id\": 2},  # DIFFERENT \u2192 separate notification\n    # ... other fields\n)\n# \u2192 New EventType (thread) for this API\n</code></pre>"},{"location":"developer_guides/notifications/#the-data-model","title":"The Data Model","text":"<p>Notifications are split into three primary models:</p> <ul> <li><code>EventType</code> (per team, per slug+event_data): The event thread key. Stores identifier, event data, and severity level.</li> <li><code>NotificationEvent</code> (per occurrence): Each call to <code>create_notification()</code> creates a new event with title/message/links.</li> <li><code>EventUser</code> (per user per event thread): Tracks read/unread status and muting for each user.</li> </ul> <p>When you call <code>create_notification()</code>, the system creates/updates these automatically, applying permission filters when determining which users receive it.</p> <p>User preferences live in <code>UserNotificationPreferences</code> and control: - In-app enabled + minimum in-app level (used for unread counts/badge) - Email enabled + minimum email level (used when sending emails) - Do Not Disturb (blocks all notifications for a duration)</p>"},{"location":"developer_guides/notifications/#how-to-create-notifications","title":"How to Create Notifications","text":""},{"location":"developer_guides/notifications/#basic-usage","title":"Basic Usage","text":"<p>Use the <code>create_notification()</code> utility function from <code>apps.ocs_notifications.utils</code>:</p> <pre><code>from apps.ocs_notifications.models import LevelChoices\nfrom apps.ocs_notifications.utils import create_notification\n\ncreate_notification(\n    title=\"Custom Action is down\",\n    message=\"The custom action 'My API' is currently unreachable.\",\n    level=LevelChoices.ERROR,\n    team=team_object,\n    slug=\"custom-action-health-check\",\n    event_data={\"action_id\": 123},\n)\n</code></pre>"},{"location":"developer_guides/notifications/#where-to-add-notification-methods","title":"Where to Add Notification Methods","text":"<p>For better code organization and maintainability, preferably add your notification helper functions in <code>apps/ocs_notifications/notifications.py</code>.</p>"},{"location":"developer_guides/notifications/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>title</code> (str): Short notification heading (shown in list)</li> <li><code>message</code> (str): Detailed notification content</li> <li><code>level</code> (LevelChoices): Severity level (INFO, WARNING, ERROR)</li> <li><code>team</code> (Team): Team to notify (determines who receives this notification)</li> <li><code>slug</code> (str): Identifier for notification type; groups related notifications</li> <li>Use format: <code>\"feature-event-type\"</code> (e.g., <code>\"custom-action-health-check\"</code>)</li> <li>Same slug + same event_data = same event thread</li> </ul>"},{"location":"developer_guides/notifications/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>event_data</code> (dict): Additional JSON data stored with the notification</li> <li>Combined with <code>slug</code> to create the deduplication identifier</li> <li>Use for: IDs, status flags, context needed for re-delivery decisions</li> <li>Best practice: Include minimal identifiers needed for deduplication logic</li> <li> <p>Default: empty dict</p> </li> <li> <p><code>permissions</code> (list[str]): Django permission codenames to filter recipients</p> </li> <li>Format: <code>\"app_label.action\"</code> (e.g., <code>\"custom_actions.change_customaction\"</code>)</li> <li>Only team members with ALL specified permissions will receive the notification</li> <li>Permissions are checked per-team (combines Django perms + team membership)</li> <li> <p>Default: None (notify all team members in the team)</p> </li> <li> <p><code>links</code> (dict): A dictionary of label \u2192 URL pairs to attach to the notification</p> </li> <li>These are rendered as clickable chips/buttons in the notification UI</li> <li>Use for: linking to the relevant bot, session, or admin page</li> <li>Example: <code>{\"View Bot\": \"/experiments/123/\", \"View Session\": \"/sessions/456/\"}</code></li> <li>Default: empty dict</li> </ul>"},{"location":"developer_guides/notifications/#delivery-behavior","title":"Delivery Behavior","text":"<ul> <li>In-app: Notifications always create <code>EventUser</code> rows for eligible users. The unread badge/count respects the   user\u2019s in-app level preference (and can be disabled entirely).</li> <li>Email: Email is sent synchronously when a notification is created and the user meets their email preferences.</li> <li>Do Not Disturb / Mute: If a user has Do Not Disturb enabled or muted that event thread, they won\u2019t be notified of new events.</li> </ul>"},{"location":"developer_guides/notifications/#when-where-to-call-create_notification","title":"When &amp; Where to Call create_notification()","text":"<p>The best place to call <code>create_notification()</code> depends on your use case. All contexts are safe:</p>"},{"location":"developer_guides/notifications/#adding-notifications-to-your-feature","title":"Adding Notifications to Your Feature","text":"<p>Follow this checklist when integrating notifications:</p> <ol> <li>Identify the event: What user-facing event should trigger the notification?</li> <li> <p>Examples: Custom action down, evaluation complete, data sync failed</p> </li> <li> <p>Choose a slug: A descriptive identifier for the notification type</p> </li> <li>Format: <code>\"feature-event-type\"</code> (e.g., <code>\"data-sync-failed\"</code>)</li> <li> <p>Use consistent slug for the same type of event</p> </li> <li> <p>Design event_data: What minimal context identifies this specific event instance?</p> </li> <li>Include IDs to distinguish between different instances (e.g., <code>action_id</code>, <code>experiment_id</code>)</li> <li>Keep it small and JSON-serializable</li> <li> <p>Don't include sensitive data (passwords, tokens, etc.)</p> </li> <li> <p>Pick a severity level: Should this be INFO, WARNING, or ERROR?</p> </li> <li>INFO: Confirmations, completions, general information</li> <li>WARNING: Performance issues, retry scenarios, unusual conditions</li> <li> <p>ERROR: System failures, unreachable services, data loss risks</p> </li> <li> <p>Determine scope: Who should receive this notification?</p> </li> <li>All team members? Or only users with specific permissions?</li> <li>Use <code>permissions</code> parameter if role-based filtering is needed</li> <li>Test with users having different permissions</li> </ol>"},{"location":"developer_guides/notifications/#span-driven-notifications-tracing-integration","title":"Span-Driven Notifications (Tracing Integration)","text":"<p>For errors that occur inside traced spans (pipeline execution, message processing, etc.), notifications are fired automatically at trace exit rather than eagerly at each call site. This is powered by <code>SpanNotificationConfig</code> and the <code>OCSTracer</code>.</p>"},{"location":"developer_guides/notifications/#how-it-works","title":"How It Works","text":"<ol> <li>When opening a span via <code>TracingService.span()</code>, pass a <code>SpanNotificationConfig</code> to declare that errors in this span should trigger a notification:</li> </ol> <pre><code>from apps.service_providers.tracing.base import SpanNotificationConfig\n\nwith self.trace_service.span(\n    \"Run Pipeline\",\n    inputs={\"input\": data},\n    notification_config=SpanNotificationConfig(permissions=[\"experiments.change_experiment\"]),\n) as span:\n    # If an exception propagates out of this block, a notification\n    # will be fired automatically when the trace completes.\n    result = do_work()\n    span.set_outputs({\"result\": result})\n</code></pre> <ol> <li>When a span with a <code>notification_config</code> exits with an error, the <code>OCSTracer</code> captures the innermost erroring span's name and config (first error wins).</li> <li>At trace exit, if an error was detected and the experiment is not a working version, <code>OCSTracer</code> fires a single <code>trace_error_notification</code> with:</li> <li>A human-readable title derived from the span name (e.g., <code>\"Run Pipeline\"</code> becomes <code>\"Run Pipeline Failed for 'My Bot'\"</code>)</li> <li>The error message from the exception</li> <li>A link to the trace record for debugging</li> <li>Permission-scoped delivery based on the <code>SpanNotificationConfig</code></li> </ol>"},{"location":"developer_guides/notifications/#when-to-use-span-driven-notifications","title":"When to Use Span-Driven Notifications","text":"<ul> <li>Preferrably always when you use a span and want users to be notified if that specific span encounters an error.</li> </ul> <p>Use direct <code>create_notification()</code> calls when: - The error occurs outside of a trace context (e.g., background tasks, health checks) - You need custom slug/event_data control beyond what span names provide</p>"},{"location":"developer_guides/slack_channel_integration/","title":"Slack Integration on Localhost","text":"<p>This guide helps you set up Slack integration with your development environment using a local server exposed via ngrok.</p>"},{"location":"developer_guides/slack_channel_integration/#1-create-a-slack-app","title":"1. Create a Slack App","text":"<ol> <li>Visit https://api.slack.com/apps and click Create New App.</li> <li>Choose From scratch.</li> <li>Provide a name for your app and select your workspace.</li> </ol>"},{"location":"developer_guides/slack_channel_integration/#2-add-bot-scopes","title":"2. Add Bot Scopes","text":"<ol> <li>Go to OAuth &amp; Permissions \u2192 Scopes.</li> <li>Under Bot Token Scopes, add the scopes that your app needs, for example:</li> <li><code>chat:write</code> \u2014 allows sending messages</li> <li><code>files:write</code> \u2014 allows uploading files</li> <li>Click Save Changes.</li> </ol>"},{"location":"developer_guides/slack_channel_integration/#3-install-the-app-to-your-workspace","title":"3. Install the App to Your Workspace","text":"<ul> <li>Navigate to Install App in the left sidebar.</li> <li>Click Install to Workspace and authorize.</li> </ul>"},{"location":"developer_guides/slack_channel_integration/#4-update-environment-variables","title":"4. Update Environment Variables","text":"<ol> <li>Go to the Basic Information section of your app.</li> <li>Copy the following values:    <code>SLACK_CLIENT_ID</code> <code>SLACK_CLIENT_SECRET</code> <code>SLACK_SIGNING_SECRET</code></li> <li>Add these to your <code>.env</code> file:</li> </ol> <pre><code>SLACK_CLIENT_ID=your-client-id\nSLACK_CLIENT_SECRET=your-client-secret\nSLACK_SIGNING_SECRET=your-signing-secret\n</code></pre>"},{"location":"developer_guides/slack_channel_integration/#5-install-ngrok","title":"5. Install ngrok","text":"<ol> <li>Download and install ngrok from https://ngrok.com/download.</li> <li>Start your Django server with a public URL using:</li> </ol> <p><pre><code>invoke runserver --public\n</code></pre> 3. This will expose your local server and generate a public HTTPS URL like:    <pre><code>https://abc123.ngrok.io\n</code></pre></p>"},{"location":"developer_guides/slack_channel_integration/#6-update-redirect-urls-in-slack-app","title":"6. Update Redirect URLs in Slack App","text":"<ol> <li>In your Slack app, go to OAuth &amp; Permissions.</li> <li>Under Redirect URLs, add:</li> </ol> <p><pre><code>https://&lt;your-ngrok-subdomain&gt;.ngrok.io/slack/oauth_redirect\n</code></pre> 3. Click Save URLs.</p>"},{"location":"developer_guides/slack_channel_integration/#7-set-up-slack-events-endpoint","title":"7. Set Up Slack Events Endpoint","text":"<ol> <li>Go to Event Subscriptions in your Slack app.</li> <li>Toggle Enable Events.</li> <li>Set the Request URL to:</li> </ol> <p><pre><code>https://&lt;your-ngrok-subdomain&gt;.ngrok.io/slack/events\n</code></pre> 4. Under Subscribe to Bot Events, add:    - <code>message.channels</code>    - <code>message.im</code>    - Click Save Changes.</p>"},{"location":"developer_guides/slack_channel_integration/#8-update-local-django-settings","title":"8. Update Local Django Settings","text":"<p>In your <code>settings.py</code>:</p> <pre><code>SITE_URL_ROOT = \"https://&lt;your-ngrok-subdomain&gt;.ngrok.io\"\n\nALLOWED_HOSTS = [\n    \"localhost\",\n    \"127.0.0.1\",\n    \"&lt;your-ngrok-subdomain&gt;.ngrok.io\"\n]\n\nCSRF_TRUSTED_ORIGINS = [\n    \"https://&lt;your-ngrok-subdomain&gt;.ngrok.io\"\n]\n</code></pre>"},{"location":"developer_guides/slack_channel_integration/#9-configure-messaging-provider-in-open-chat-studio","title":"9. Configure Messaging Provider in Open Chat Studio","text":"<ol> <li>Go to Team Settings \u2192 Messaging Providers.</li> <li>Click Add, and select Slack as the provider type.</li> <li>Click Connect Slack \u2014 you'll be redirected to Slack's authorization screen.</li> </ol>"},{"location":"developer_guides/slack_channel_integration/#10-authorize-in-slack","title":"10. Authorize in Slack","text":"<ul> <li>Slack may auto-select a workspace. If the correct workspace isn't shown:  Open the link in Incognito mode or clear cookies.</li> <li>You'll be redirected to your ngrok URL (not <code>localhost</code>).</li> <li>Log back into Open Chat Studio if prompted.</li> <li>Navigate again to Team Settings \u2192 Messaging Providers and complete the Slack setup.</li> </ul>"},{"location":"developer_guides/slack_channel_integration/#done","title":"\u2705 Done!","text":"<p>Your Slack app is now integrated with your local development environment!</p>"},{"location":"developer_guides/twilio_testing/","title":"Testing Twilio Integration Locally","text":"<p>This guide covers setting up Twilio WhatsApp integration for local development.</p>"},{"location":"developer_guides/twilio_testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Twilio account with a phone number (or Twilio Sandbox for WhatsApp)</li> <li>ngrok installed (https://ngrok.com/download)</li> </ul>"},{"location":"developer_guides/twilio_testing/#1-expose-local-server-with-ngrok","title":"1. Expose Local Server with ngrok","text":"<p>Start your Django server with a public URL:</p> <pre><code>invoke runserver --public\n</code></pre> <p>This generates a public HTTPS URL like <code>https://abc123.ngrok.io</code>.</p>"},{"location":"developer_guides/twilio_testing/#2-create-messaging-provider","title":"2. Create Messaging Provider","text":"<ol> <li>Go to Team Settings \u2192 Messaging Providers</li> <li>Click Add and select Twilio as the provider type</li> <li>Enter your Twilio Account SID and Auth Token</li> </ol>"},{"location":"developer_guides/twilio_testing/#3-create-channel-on-bot","title":"3. Create Channel on Bot","text":"<ol> <li>Navigate to your bot/experiment</li> <li>Go to Channels and add a new WhatsApp channel</li> <li>Select your Twilio messaging provider</li> <li>Note the webhook URL displayed</li> </ol>"},{"location":"developer_guides/twilio_testing/#4-configure-twilio-webhook","title":"4. Configure Twilio Webhook","text":"<ol> <li>Go to Twilio Console</li> <li>Navigate to your phone number settings (Messaging -&gt; Senders -&gt; WhatsApp senders)</li> <li>Under Messaging Endpoint Configuration, set the webhook URL:</li> <li>URL format: <code>https://&lt;your-ngrok-subdomain&gt;.ngrok.io/channels/twilio/&lt;channel-id&gt;/incoming/</code></li> <li>Set the HTTP method to POST</li> </ol> <p>Important: The callback URL must use your ngrok domain, not localhost.</p>"},{"location":"developer_guides/twilio_testing/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Webhook not receiving messages: Verify ngrok is running and the URL matches exactly</li> <li>Signature validation errors: Ensure the Auth Token in OCS matches your Twilio account and that Django is using HTTPS for generating abosulte URLs.</li> <li>Voice transcription failing: Check voice provider credentials and that the provider supports transcription</li> </ul>"},{"location":"developer_guides/user_docs/","title":"User Documentation and Changelog Process","text":"<p>User documentation and the user facing changelog are hosted in the doc repo and published to https://docs.openchatstudio.com/.</p> <p>In principle, all user-facing changes should be accompanied by documentation updates and a changelog but discretion should be used. For example, if a change is purely internal and doesn't affect the user experience, it may not need to be included or if it is a very minor change.</p>"},{"location":"developer_guides/user_docs/#changelog-process","title":"Changelog process","text":"<p>Ideally, when creating a PR, also create a PR in the docs repo with any documentation updates and a changelog entry and then add a link to the docs PR in your code changes PR.</p> <p>Changelog entries should be brief but should link to any relevant documentation for further details.</p>"},{"location":"developer_guides/user_docs/#changelog-summaries","title":"Changelog summaries","text":"<p>Once a week (currently on a Monday) a GitHub actions workflow runs and generates a release in the docs repo with a summary of the changes since the previous release. This creates a way for users to get notified of changes by subscribing to the release feed.</p> <p>The automated releases are created in <code>draft</code> state which allows a developer to review the generated text before publishing. The releases should contain the following sections:</p> <ul> <li>New Features: new features added to the prodcut</li> <li>Improvements: changes to existing features that don't classify as 'new features'</li> <li>Bug Fixes</li> </ul> <p>It should not contain a top level summary, upgrade recommendations, etc.</p> <p>The process for manually reviewing and publishing a release is:</p> <ol> <li>Review the repo diff between this release and the previous release. You can access this by using the 'Compare' drop down in the left sidebar. This is a good idea to ensure that the release notes are accurate and complete.</li> <li>Review the previous release notes to see if there are any items that have already been included in a previous release.</li> <li>If there are docs to link to for any item, ensure that they are added.</li> <li>If you think there should be docs where there aren't, either create them immediately or create a ticket to be prioritized later.</li> </ol> <p>Once you are happy with the release notes, publish the release. This will send a notification to all users who are subscribed to the release feed.</p>"},{"location":"developer_guides/user_docs/#api-documentation","title":"API Documentation","text":"<p>The OCS REST API is primarily documented via its OpenAPI schema. The schema is created using drf-spectacular.</p> <p>The current production schema is available at https://chatbots.dimagi.com/api/schema/. It is also kept in the code repository in the <code>api-schema.yml</code> file. This file serves two purposes:</p> <ol> <li>Provide an easy way to visually inspect changes to the schema.</li> <li>Provide a reference for generating API documentation in the docs repo (see below).</li> </ol> <p>The schema can be generated locally by running:</p> <pre><code>inv schema\n# OR\npython manage.py spectacular --file api-schema.yml --validate\n</code></pre>"},{"location":"developer_guides/user_docs/#api-schema-updates","title":"API Schema updates","text":"<p>Whenever changes are made that impact the API schema, the <code>api-schema.yml</code> file must also be updated. This is enforced by a test which will fail if the schema file is out of date. Ensuring that this file is up to date also allows us to it as a trigger for updating the API docs in the docs repo:</p> <ol> <li><code>api-schema.yml</code> file changes in the <code>main</code> branch.</li> <li><code>api-schema-dispatch.yml</code> GitHub action runs which sends a dispatch event to the OCS docs repo.</li> <li>A GitHub action in the OCS docs repo runs and creates a PR with any updated API docs.</li> </ol>"},{"location":"developer_guides/versioning/","title":"Versioning Dev Documentation","text":"<p>See also the User Documentation on versioning.</p>"},{"location":"developer_guides/versioning/#versioning-terminology","title":"Versioning Terminology","text":"Term User-Facing Term Description Working version Unreleased version The editable versio of an object. Default version Published version The version currently live and user facing. Version family \u2014 A group of instances that are versions of the same working instance. This includes the working version itself"},{"location":"developer_guides/versioning/#how-to-think-about-versioning","title":"How to Think About Versioning","text":"<p>Users are always working on the latest version of their chatbot. When they create a new version, it is really only freezing their progress and assigning a version number to it. Any new edits will be made on the next version.</p> <ul> <li>All versioned objects have a <code>working_version</code> field, which is a foreign key to an instance of the same model.</li> <li>Creating a new version means duplicating the <code>working version</code>. All objects that contribute to the behavior of the chatbot are also versioned and linked to the new version. The exception to this is global objects such as LLM providers which are never versioned.</li> <li>The duplicated object\u2019s <code>working_version</code> points to the original object.</li> <li>For objects like Experiments and Pipelines which have version numbers, the following applies:<ul> <li>The newly created version gets the current version number.</li> <li>The working version's version number is incremented.</li> </ul> </li> </ul>"},{"location":"developer_guides/versioning/#how-to-version-a-model","title":"How to Version a Model","text":"<p>If you add a model that needs to be versioned, you generally need to do the following:</p> <ol> <li> <p>Inherit from mixins:</p> <ul> <li><code>VersionsMixin</code> - Adds utility methods for working with versions.</li> <li><code>VersionsObjectManagerMixin</code> - This ensures that archived objects are excluded by default and helpful annotations are added to querysets of your model.</li> </ul> </li> <li> <p>Add these fields to your model:</p> <ul> <li><code>working_version</code>: A nullable foreign key to itself</li> <li><code>is_archived</code>: A <code>BooleanField</code> indicating whether or not this instance is archived</li> <li><code>version_number</code>: (optional) `IntegerField' used to track the objects version number. This is only really necessary for top level objects.</li> </ul> </li> <li> <p>Implement <code>version_details</code>: See the VersionDetails section</p> </li> <li>Filter returned objects to the UI: Be sure to only return working versions to users.</li> </ol> <p>If the model can be linked to a Pipeline Node, be sure to version this object whenever the node is versioned. The same goes for archiving. Whenever this node is archived and it is a versioned node, we need to make sure that all related versioned models are archived as well.</p>"},{"location":"developer_guides/versioning/#the-versiondetails-class","title":"The <code>VersionDetails</code> class","text":"<p>The <code>VersionDetails</code> class is a core component of the versioning system that encapsulates the version-specific information of a model instance. It consists of:</p> <ul> <li><code>instance</code>: The model instance being versioned</li> <li><code>fields</code>: A list of <code>VersionField</code> objects representing the versioned fields</li> <li><code>fields_changed</code>: A boolean indicating if any fields have changed</li> <li><code>fields_grouped</code>: Property that groups fields by their <code>group_name</code> for organized display</li> </ul> <p>Key methods:</p> <ul> <li><code>compare(previous_version_details)</code>: Compares the current instance with a previous version, tracking changes</li> <li><code>get_field(field_name)</code>: Retrieves a specific field by name</li> </ul> <p>Each <code>VersionField</code> represents a field or property of the model and can handle:</p> <ul> <li>Primitive values (strings, numbers, etc.)</li> <li>Querysets (collections of related objects)</li> <li>Versioned models (models that implement <code>version_details</code>)</li> <li>Unversioned models (regular Django models)</li> </ul> <p>When implementing versioning for a model, you must provide a <code>version_details</code> property that returns a <code>VersionDetails</code> instance with the fields you want to track.</p> <p>Example: <pre><code>def _get_version_details(self) -&gt; VersionDetails:\n    return VersionDetails(\n        instance=self,\n        fields=[\n            VersionField(group_name=\"General\", name=\"name\", raw_value=self.name),\n            VersionField(group_name=\"Settings\", name=\"enabled\", raw_value=self.enabled),\n        ]\n    )\n</code></pre></p>"},{"location":"developer_guides/versioning/#archiving","title":"Archiving","text":"<p>An object can only be archived if neither its working version nor any of its other versions are used by any published version of a related object.</p> <p>If the object is in use, the user should be informed of the specific usages and prompted to archive those related objects first. For reference, see the behavior when attempting to archive an <code>OpenAiAssistant</code>.</p>"},{"location":"developer_guides/waf_management/","title":"WAF Management","text":"<p>Open Chat Studio uses AWS WAF (Web Application Firewall) to protect against common web exploits. This guide explains how to manage WAF rules and exceptions for legitimate application endpoints.</p>"},{"location":"developer_guides/waf_management/#overview","title":"Overview","text":"<p>The WAF management system consists of three components:</p> <ol> <li><code>@waf_allow</code> decorator - Marks views that need WAF rule exceptions</li> <li><code>export_waf_allow_list</code> command - Generates WAF rule configurations</li> <li><code>filter_valid_paths.py</code> script - Analyzes WAF/load balancer logs</li> </ol>"},{"location":"developer_guides/waf_management/#waf-rules","title":"WAF Rules","text":"<p>Open Chat Studio currently defines two WAF rule exceptions:</p>"},{"location":"developer_guides/waf_management/#sizerestrictions_body","title":"SizeRestrictions_BODY","text":"<p>Bypasses body size limits for endpoints that accept large POST bodies (file uploads, document processing, etc.)</p>"},{"location":"developer_guides/waf_management/#nouseragent_header","title":"NoUserAgent_HEADER","text":"<p>Allows requests without User-Agent headers for endpoints accessed by bots, webhooks, or API clients</p>"},{"location":"developer_guides/waf_management/#marking-views-with-waf_allow","title":"Marking Views with <code>@waf_allow</code>","text":"<p>Use the <code>@waf_allow</code> decorator to mark views that need WAF rule exceptions.</p>"},{"location":"developer_guides/waf_management/#usage","title":"Usage","text":"<pre><code>from apps.web.waf import waf_allow, WafRule\n\n# Function-based view\n@waf_allow(WafRule.SizeRestrictions_BODY)\ndef upload_file(request):\n    # Handle large file uploads\n    pass\n\n# Class-based view\n@waf_allow(WafRule.NoUserAgent_HEADER)\nclass WebhookView(View):\n    # Handle webhook requests that may not send User-Agent\n    pass\n</code></pre>"},{"location":"developer_guides/waf_management/#important-notes","title":"Important Notes","text":"<ul> <li>The <code>@waf_allow</code> decorator MUST be the topmost decorator on the function or class</li> <li>For class-based views, apply it to the class itself, not to methods</li> <li>Only use when necessary - most views should go through full WAF protection</li> </ul>"},{"location":"developer_guides/waf_management/#examples","title":"Examples","text":"<pre><code># \u2705 Correct - topmost decorator on class\n@waf_allow(WafRule.SizeRestrictions_BODY)\nclass DocumentUploadView(LoginAndTeamRequiredMixin, CreateView):\n    model = Document\n    # ...\n\n# \u2705 Correct - topmost decorator on function\n@waf_allow(WafRule.NoUserAgent_HEADER)\n@csrf_exempt\ndef telegram_webhook(request, channel_external_id):\n    # ...\n\n# \u274c Incorrect - decorator below other decorators\n@login_required\n@waf_allow(WafRule.SizeRestrictions_BODY)\ndef my_view(request):\n    # This won't work correctly\n    pass\n</code></pre>"},{"location":"developer_guides/waf_management/#exporting-waf-rules","title":"Exporting WAF Rules","text":"<p>After adding <code>@waf_allow</code> decorators, generate the updated WAF configuration:</p> <pre><code>python manage.py export_waf_allow_list\n</code></pre>"},{"location":"developer_guides/waf_management/#output-format","title":"Output Format","text":"<p>The command generates Python code ready for the <code>ocs-deploy</code> repository:</p> <pre><code># URI patterns for endpoints that can send large POST bodies\n# These bypass only SizeRestrictions_BODY, all other protections remain active\nSizeRestrictions_BODY = [\n    r\"^a/[a-z0-9_-]+/assistants/new/$\",\n    r\"^a/[a-z0-9_-]+/documents/collections/\\d+/add_files$\",\n    r\"^slack/events$\",\n]\n\n# URI patterns for endpoints that may not send User-Agent header\n# These bypass only NoUserAgent_HEADER, all other protections remain active\nNoUserAgent_HEADER = [\n    r\"^a/[a-z0-9_-]+/chatbots/[^/]+/start/$\",\n    r\"^channels/telegram/[^/]+$\",\n]\n</code></pre>"},{"location":"developer_guides/waf_management/#deployment","title":"Deployment","text":"<ol> <li>Run the export command</li> <li>Copy the output into the <code>ocs-deploy</code> repository's WAF module</li> <li>Deploy the updated WAF configuration</li> </ol>"},{"location":"developer_guides/waf_management/#analyzing-waf-logs","title":"Analyzing WAF Logs","text":"<p>Use <code>scripts/filter_valid_paths.py</code> to analyze AWS WAF or load balancer logs and identify which blocked requests are legitimate views. You still need to review the matches since many will be valid rule matches ie. requests that we do want to block.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section will help you set up your development environment and get started with Open Chat Studio.</p>"},{"location":"getting-started/#development-environment-setup","title":"Development Environment Setup","text":"<p>Open Chat Studio uses UV and Invoke for dev automation.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13 (recommended)</li> <li>Node.js &gt;= 24.0.0</li> <li>Docker and Docker Compose</li> <li>Git</li> </ul>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/dimagi/open-chat-studio.git\ncd open-chat-studio\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code>uv venv --python 3.13\nsource .venv/bin/activate\nuv sync\n</code></pre> </li> <li> <p>Run the automated setup</p> <pre><code>inv setup-dev-env\n</code></pre> <p>This will: - Install pre-commit hooks - Start database and Redis services - Run database migrations - Build frontend resources - Create a superuser</p> Manual steps </li> <li> <p>Start the development server</p> <pre><code>./manage.py runserver\n</code></pre> </li> <li> <p>Run Celery for background tasks</p> <p>Celery is required to handle LLM interactions. Run it using:</p> <pre><code>inv celery\n</code></pre> <p>For a production-like setup, use:</p> <pre><code>inv celery --gevent\n</code></pre> </li> </ol>"},{"location":"getting-started/#install-the-pre-commit-hooks","title":"Install the pre-commit hooks","text":"<pre><code>prek install --install-hooks\n</code></pre>"},{"location":"getting-started/#set-up-database","title":"Set up database","text":"<p>Start the database and redis services and run the DB migrations:</p> <pre><code>inv up  # start the docker services\ncp .env.example .env\n./manage.py migrate\n</code></pre>"},{"location":"getting-started/#build-the-front-end-resources","title":"Build the front-end resources","text":"<p>To build JavaScript and CSS files, first install npm packages:</p> <pre><code>inv npm --install\n# or\nnpm install\nnpm run dev\n</code></pre> <p>Note</p> <p>You should be using node &gt;= 24.0.0. If you have nvm installed, you can run <code>nvm use</code> to switch to the correct version.</p> <p>To check which version you are using use <code>node --version</code>.</p>"},{"location":"getting-started/#create-a-superuser","title":"Create a superuser","text":"<pre><code>./manage.py createsuperuser\n</code></pre>"},{"location":"getting-started/#docker-only-development-environment","title":"Docker-Only Development Environment","text":"<p>As an alternative to running Django and Celery on the host, you can run the full stack inside Docker. This requires only Docker \u2014 no local Python or Node installation needed.</p>"},{"location":"getting-started/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> </ul>"},{"location":"getting-started/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/dimagi/open-chat-studio.git\ncd open-chat-studio\n</code></pre> </li> <li> <p>Create your <code>.env</code> file</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> and set at minimum <code>SECRET_KEY</code>. The <code>DATABASE_URL</code> and <code>REDIS_URL</code> values in <code>.env</code> are ignored when running via Docker Compose \u2014 those are overridden by the compose file to use the container service names.</p> </li> <li> <p>Build the images</p> <pre><code>docker compose build\n</code></pre> </li> <li> <p>Start everything</p> <pre><code>docker compose up\n</code></pre> <p>On first start, Docker Compose will: - Start PostgreSQL and wait until it is healthy - Start Redis - Run <code>python manage.py migrate</code> (the <code>migrate</code> service exits once complete) - Start the Django dev server on http://localhost:8000 - Start a Celery worker and Celery Beat scheduler</p> </li> <li> <p>Create a superuser</p> <p>In a separate terminal:</p> <pre><code>docker compose run --rm web python manage.py createsuperuser\n</code></pre> </li> </ol>"},{"location":"getting-started/#services","title":"Services","text":"Service Description <code>db</code> PostgreSQL with pgvector extension <code>redis</code> Redis (used as Celery broker and result backend) <code>migrate</code> Runs <code>manage.py migrate</code> on startup, then exits <code>web</code> Django dev server with auto-reload (<code>runserver</code>) <code>celery_worker</code> Celery worker for background tasks <code>celery_beat</code> Celery Beat scheduler (uses <code>django_celery_beat</code> database scheduler)"},{"location":"getting-started/#useful-commands","title":"Useful commands","text":"<p>Run a management command:</p> <pre><code>docker compose run --rm web python manage.py &lt;command&gt;\n</code></pre> <p>View logs for a specific service:</p> <pre><code>docker compose logs -f web\ndocker compose logs -f celery_worker\n</code></pre> <p>Rebuild after dependency changes (<code>pyproject.toml</code> / <code>uv.lock</code>):</p> <pre><code>docker compose build\ndocker compose up\n</code></pre> <p>Stop all services and remove containers:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#type-halfvec-does-not-exist-during-migrations","title":"<code>type \"halfvec\" does not exist</code> during migrations","text":"<p>This error means the <code>pgvector</code> extension in your PostgreSQL container is too old. The <code>halfvec</code> type requires pgvector \u2265 0.7.0.</p> <p>Cause: Your locally cached <code>pgvector/pgvector:pg16</code> Docker image predates pgvector 0.7.0.</p> <p>Fix (fresh setup \u2014 no data to keep):</p> <pre><code>docker compose down -v      # removes containers and volumes\ndocker compose pull db       # fetch the latest pgvector image\ndocker compose up            # reinitialise and re-run migrations\n</code></pre> <p>Fix (existing data to preserve):</p> <pre><code>docker compose pull db\ndocker compose up -d --force-recreate db\ndocker compose exec db psql -U postgres -d open_chat_studio -c \"ALTER EXTENSION vector UPDATE;\"\ndocker compose run --rm migrate\n</code></pre>"},{"location":"getting-started/#database-open_chat_studio-does-not-exist","title":"Database <code>open_chat_studio</code> does not exist","text":"<p>PostgreSQL only creates the database named in <code>POSTGRES_DB</code> when initialising a fresh data volume. If a <code>postgres_data</code> volume already exists from a previous run without the database, you will see this error.</p> <p>Fix:</p> <pre><code>docker compose exec db createdb -U postgres open_chat_studio\ndocker compose run --rm migrate\n</code></pre> <p>Or to start completely fresh (deletes all data):</p> <pre><code>docker compose down -v\ndocker compose up\n</code></pre>"},{"location":"getting-started/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"getting-started/#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre> <p>Or to test a specific app/module:</p> <pre><code>pytest apps/utils/tests/test_slugs.py\n</code></pre>"},{"location":"getting-started/#updating-translations","title":"Updating Translations","text":"<pre><code>inv translations\n</code></pre>"},{"location":"getting-started/#linting-and-formatting","title":"Linting and Formatting","text":"<p>The project uses ruff for linting and formatting:</p> <pre><code>inv ruff\n</code></pre>"},{"location":"getting-started/#updating-requirements","title":"Updating Requirements","text":"<pre><code>inv requirements\n</code></pre> <p>To add a new requirement:</p> <pre><code>uv add &lt;package-name&gt;\n\n# for dev / prod dependencies\nuv add &lt;package-name&gt; --group [dev|prod]\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-design/","title":"Incremental ty Type Checking Adoption","text":"<p>Date: 2026-02-18 Status: Approved</p>"},{"location":"plans/2026-02-18-ty-type-checking-design/#goal","title":"Goal","text":"<p>Gradually adopt ty (Astral's Rust-based Python type checker) across the Open Chat Studio codebase, working toward full type coverage over time.</p>"},{"location":"plans/2026-02-18-ty-type-checking-design/#context","title":"Context","text":"<ul> <li>1,209 Python files across 36 Django apps</li> <li>No existing type checker configured</li> <li>Minimal existing type annotations (76 files import from <code>typing</code>, 17 use <code>from __future__ import annotations</code>)</li> <li>No <code>django-stubs</code> or <code>django-types</code> installed</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#baseline-ty-diagnostics-v0017-all-defaults","title":"Baseline ty diagnostics (v0.0.17, all defaults)","text":"Rule Count Notes <code>unresolved-attribute</code> 2,417 Mostly Django <code>.objects</code> manager <code>invalid-argument-type</code> 369 Mix of real issues and Django false positives <code>invalid-assignment</code> 109 <code>not-subscriptable</code> 107 <code>invalid-parameter-default</code> 79 <code>invalid-method-override</code> 46 Django metaclass patterns <code>possibly-missing-attribute</code> 43 <code>invalid-return-type</code> 33 <code>unsupported-operator</code> 31 <code>invalid-type-form</code> 29 Other rules &lt; 15 each Total 3,332"},{"location":"plans/2026-02-18-ty-type-checking-design/#key-constraint","title":"Key constraint","text":"<p>ty is in beta and does not support Django plugins (no plans to add a plugin system). The <code>django-types</code> package (plugin-free stubs forked from <code>django-stubs</code>) is the recommended workaround.</p>"},{"location":"plans/2026-02-18-ty-type-checking-design/#design","title":"Design","text":""},{"location":"plans/2026-02-18-ty-type-checking-design/#strategy-hybrid-rule-tiers-progressive-enablement","title":"Strategy: Hybrid rule tiers + progressive enablement","text":"<p>Start with all rules ignored, install <code>django-types</code> to reduce false positives, then enable rules in tiers from lowest-noise to highest-noise. Each tier is a series of small PRs.</p>"},{"location":"plans/2026-02-18-ty-type-checking-design/#phase-0-foundation-1-pr","title":"Phase 0: Foundation (1 PR)","text":"<ul> <li>Install <code>django-types</code> as a dev dependency</li> <li>Add <code>ty</code> as a dev dependency</li> <li>Create <code>[tool.ty]</code> config in <code>pyproject.toml</code>:</li> <li>Set <code>all = \"ignore\"</code> (start from zero)</li> <li>Exclude <code>migrations/</code> directory</li> <li>Add non-blocking CI step to <code>lint_and_test.yml</code> (<code>continue-on-error: true</code>)</li> <li>Add <code>ty check</code> to AGENTS.md useful commands</li> <li>Re-measure baseline with <code>django-types</code> installed</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#phase-1-high-value-low-noise-rules-small-prs","title":"Phase 1: High-value, low-noise rules (small PRs)","text":"<p>Enable rules that catch real bugs with few false positives. Each rule or small group = 1 PR.</p> <ul> <li><code>unresolved-import</code> (5 errors)</li> <li><code>unresolved-reference</code> / <code>unresolved-global</code></li> <li><code>invalid-raise</code></li> <li><code>call-non-callable</code></li> <li><code>missing-argument</code> / <code>too-many-positional-arguments</code></li> <li><code>duplicate-base</code>, <code>cyclic-class-definition</code></li> <li>Other rules with 0-5 violations</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#phase-2-medium-noise-rules-larger-prs-may-need-ty-ignore","title":"Phase 2: Medium-noise rules (larger PRs, may need <code># ty: ignore</code>)","text":"<ul> <li><code>invalid-return-type</code> (~33 errors)</li> <li><code>invalid-assignment</code> (~109 errors)</li> <li><code>unsupported-operator</code> (~31 errors)</li> <li><code>invalid-parameter-default</code> (~79 errors)</li> <li><code>not-subscriptable</code> (~107 errors, likely many from Django/third-party)</li> </ul> <p>Some violations will require <code># ty: ignore[rule]</code> inline comments for legitimate Django patterns.</p>"},{"location":"plans/2026-02-18-ty-type-checking-design/#phase-3-high-noise-rules","title":"Phase 3: High-noise rules","text":"<ul> <li><code>invalid-argument-type</code> (~369 errors, many Django false positives)</li> <li><code>unresolved-attribute</code> (~2,417 errors, depends on <code>django-types</code> reduction)</li> <li><code>invalid-method-override</code> (~46 errors, Django metaclass patterns)</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#phase-4-make-ci-blocking","title":"Phase 4: Make CI blocking","text":"<ul> <li>Remove <code>continue-on-error: true</code> from the ty CI step</li> <li>ty violations now block PRs</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#suppression-strategy","title":"Suppression strategy","text":"<ul> <li>Prefer fixing real issues over suppressing</li> <li>Use <code># ty: ignore[rule-name]</code> for known false positives (Django metaprogramming)</li> <li>Use <code>@no_type_check</code> sparingly for functions that are fundamentally untyped</li> <li>Configure rule severity in <code>[tool.ty.rules]</code> \u2014 never use blanket <code># type: ignore</code></li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-design/#ci-integration","title":"CI integration","text":"<p>Add a new job to <code>.github/workflows/lint_and_test.yml</code>:</p> <pre><code>type-check:\n  if: github.ref != 'refs/heads/main'\n  runs-on: ubuntu-latest\n  continue-on-error: true\n  steps:\n    - uses: actions/checkout@v6\n    - uses: astral-sh/setup-uv@v7\n    - name: Install dependencies\n      run: |\n        uv venv\n        uv sync --locked --dev\n    - name: Run ty\n      run: uv run ty check apps/\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-design/#sources","title":"Sources","text":"<ul> <li>ty documentation</li> <li>ty GitHub repo</li> <li>ty rules reference</li> <li>Django plugin support discussion</li> <li>django-types on PyPI</li> </ul>"},{"location":"plans/2026-02-18-ty-type-checking-plan/","title":"ty Type Checking Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Incrementally adopt ty type checking across the Open Chat Studio codebase, starting with foundation tooling and progressively enabling rules from low-noise to high-noise.</p> <p>Architecture: Install <code>django-types</code> for Django stub support, configure ty in <code>pyproject.toml</code> with all rules ignored, add non-blocking CI, then enable rules tier-by-tier in separate PRs. Each rule enablement PR fixes all violations for that rule.</p> <p>Tech Stack: ty (Python type checker), django-types (Django stubs), uv (package manager), GitHub Actions CI</p> <p>Design doc: <code>docs/plans/2026-02-18-ty-type-checking-design.md</code></p>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-1-install-django-types-and-ty-as-dev-dependencies","title":"Task 1: Install django-types and ty as dev dependencies","text":"<p>Files: - Modify: <code>pyproject.toml:139-155</code> (dev dependency group)</p> <p>Step 1: Add dependencies</p> <p>Add <code>django-types</code> and <code>ty</code> to the <code>[dependency-groups] dev</code> list in <code>pyproject.toml</code>:</p> <pre><code>[dependency-groups]\ndev = [\n    \"ruff\",\n    \"mock\",\n    \"invoke\",\n    \"termcolor\",\n    \"time-machine\",\n    \"watchfiles\",\n    \"pytest\",\n    \"pytest-django\",\n    \"factory-boy\",\n    \"pytest-httpx\",\n    \"pytest-cov\",\n    \"pytest-xdist\",\n    \"django-debug-toolbar&gt;=5.2.0\",\n    \"djlint&gt;=1.36.4\",\n    \"prek&gt;=0.3.2\",\n    \"django-types\",\n    \"ty\",\n]\n</code></pre> <p>Step 2: Lock and sync</p> <p>Run: <pre><code>uv lock &amp;&amp; uv sync --dev\n</code></pre> Expected: lock file updated, both packages installed.</p> <p>Step 3: Verify installation</p> <p>Run: <pre><code>uv run ty --version\n</code></pre> Expected: prints ty version (e.g., <code>ty 0.0.17</code> or newer).</p> <p>Step 4: Re-measure baseline with django-types</p> <p>Run: <pre><code>uv run ty check apps/ 2&gt;&amp;1 | grep -oP 'rule `[^`]+`' | sort | uniq -c | sort -rn | head -20\nuv run ty check apps/ 2&gt;&amp;1 | tail -3\n</code></pre></p> <p>Record the new diagnostic counts. The <code>unresolved-attribute</code> count should decrease now that django-types provides <code>.objects</code> stubs.</p> <p>Step 5: Commit</p> <pre><code>git add pyproject.toml uv.lock\ngit commit -m \"build: add django-types and ty as dev dependencies\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-2-configure-ty-in-pyprojecttoml-with-all-rules-ignored","title":"Task 2: Configure ty in pyproject.toml with all rules ignored","text":"<p>Files: - Modify: <code>pyproject.toml</code> (add <code>[tool.ty]</code> section after <code>[tool.djlint]</code>)</p> <p>Step 1: Add ty configuration</p> <p>Add this section at the end of <code>pyproject.toml</code> (after the <code>[tool.djlint]</code> block):</p> <pre><code>[tool.ty]\npython-version = \"3.13\"\n\n[tool.ty.src]\nexclude = [\"migrations/\"]\n\n[tool.ty.rules]\nall = \"ignore\"\n</code></pre> <p>This starts with every rule turned off. We'll enable them one-by-one in later tasks.</p> <p>Step 2: Verify ty runs clean</p> <p>Run: <pre><code>uv run ty check apps/\n</code></pre> Expected: <code>All checks passed!</code> (since all rules are ignored).</p> <p>Step 3: Commit</p> <pre><code>git add pyproject.toml\ngit commit -m \"build: add ty configuration with all rules ignored\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-3-add-non-blocking-ty-ci-step","title":"Task 3: Add non-blocking ty CI step","text":"<p>Files: - Modify: <code>.github/workflows/lint_and_test.yml</code> (add new job after <code>code-style</code>)</p> <p>Step 1: Add type-check job</p> <p>Insert this job after the <code>code-style</code> job (after line 94) in <code>.github/workflows/lint_and_test.yml</code>:</p> <pre><code>  type-check:\n    if: github.ref != 'refs/heads/main'\n    runs-on: ubuntu-latest\n    continue-on-error: true\n    steps:\n      - uses: actions/checkout@v6\n      - name: Set up Python\n        uses: actions/setup-python@v6\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n      - uses: astral-sh/setup-uv@v7\n        with:\n          enable-cache: true\n      - name: Install dependencies\n        run: |\n          uv venv\n          uv sync --locked --dev\n      - name: Run ty type checker\n        run: uv run ty check apps/\n</code></pre> <p>Key details: - <code>continue-on-error: true</code> makes this non-blocking (PR checks won't fail) - Uses the same Python version and uv setup pattern as the <code>python-tests</code> job - Only runs on PRs (not on main pushes) via the <code>if</code> condition</p> <p>Step 2: Commit</p> <pre><code>git add .github/workflows/lint_and_test.yml\ngit commit -m \"ci: add non-blocking ty type check step\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-4-update-agentsmd-with-ty-command","title":"Task 4: Update AGENTS.md with ty command","text":"<p>Files: - Modify: <code>AGENTS.md:43-53</code> (Useful commands section)</p> <p>Step 1: Add ty check command</p> <p>Add this line after the existing lint/format commands (after \"Format python\" line):</p> <pre><code>* Type check python: `ty check apps/`\n</code></pre> <p>Step 2: Commit</p> <pre><code>git add AGENTS.md\ngit commit -m \"docs: add ty check to AGENTS.md useful commands\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-5-enable-phase-1-low-noise-rules-unresolved-import","title":"Task 5: Enable Phase 1 low-noise rules \u2014 unresolved-import","text":"<p>Files: - Modify: <code>pyproject.toml</code> (<code>[tool.ty.rules]</code> section) - Fix: any files with <code>unresolved-import</code> violations</p> <p>Step 1: Check current violations</p> <p>Run: <pre><code>uv run ty check --ignore all --error unresolved-import apps/ 2&gt;&amp;1\n</code></pre></p> <p>The pre-django-types baseline had 5 violations. Record the new count.</p> <p>Step 2: Enable the rule</p> <p>In <code>pyproject.toml</code>, update the <code>[tool.ty.rules]</code> section:</p> <pre><code>[tool.ty.rules]\nall = \"ignore\"\nunresolved-import = \"error\"\n</code></pre> <p>Step 3: Fix violations</p> <p>For each violation, either: - Fix the import if it's a real issue - Add <code># ty: ignore[unresolved-import]</code> if it's a conditional/optional import (e.g., <code>try: import mailchimp3</code>)</p> <p>Step 4: Verify clean</p> <p>Run: <pre><code>uv run ty check apps/\n</code></pre> Expected: <code>All checks passed!</code></p> <p>Step 5: Commit</p> <pre><code>git add -u\ngit commit -m \"types: enable unresolved-import rule and fix violations\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-6-enable-phase-1-low-noise-rules-batch-of-zeronear-zero-violation-rules","title":"Task 6: Enable Phase 1 low-noise rules \u2014 batch of zero/near-zero violation rules","text":"<p>Files: - Modify: <code>pyproject.toml</code> (<code>[tool.ty.rules]</code> section)</p> <p>Step 1: Identify zero-violation rules</p> <p>Run each rule individually to find ones with 0 violations:</p> <pre><code>for rule in unresolved-reference unresolved-global invalid-raise call-non-callable duplicate-base cyclic-class-definition cyclic-type-alias-definition conflicting-declarations conflicting-metaclass invalid-base inconsistent-mro static-assert-error subclass-of-final-class; do\n  count=$(uv run ty check --ignore all --error \"$rule\" apps/ 2&gt;&amp;1 | grep -oP 'Found \\K\\d+' || echo \"0\")\n  echo \"$count $rule\"\ndone\n</code></pre> <p>Step 2: Enable all zero-violation rules at once</p> <p>Add them all to <code>[tool.ty.rules]</code> in <code>pyproject.toml</code>:</p> <pre><code>[tool.ty.rules]\nall = \"ignore\"\nunresolved-import = \"error\"\n# Add each zero-violation rule here as = \"error\"\n</code></pre> <p>Step 3: Verify clean</p> <p>Run: <pre><code>uv run ty check apps/\n</code></pre> Expected: <code>All checks passed!</code></p> <p>Step 4: Commit</p> <pre><code>git add pyproject.toml\ngit commit -m \"types: enable batch of zero-violation ty rules\"\n</code></pre>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-7-enable-phase-1-low-noise-rules-remaining-low-count-rules","title":"Task 7: Enable Phase 1 low-noise rules \u2014 remaining low-count rules","text":"<p>Files: - Modify: <code>pyproject.toml</code> (<code>[tool.ty.rules]</code> section) - Fix: files with violations for rules with 1-15 total violations</p> <p>Step 1: Check violation counts</p> <p>Run each remaining rule to find ones with &lt;=15 violations:</p> <pre><code>for rule in missing-argument too-many-positional-arguments unknown-argument invalid-key not-iterable no-matching-overload deprecated empty-body missing-typed-dict-key call-non-callable invalid-raise; do\n  count=$(uv run ty check --ignore all --error \"$rule\" apps/ 2&gt;&amp;1 | grep -oP 'Found \\K\\d+' || echo \"0\")\n  echo \"$count $rule\"\ndone\n</code></pre> <p>Step 2: Enable one rule at a time, fix violations, verify clean</p> <p>For each rule with &lt;=15 violations, repeat: 1. Add <code>rule-name = \"error\"</code> to <code>[tool.ty.rules]</code> 2. Run <code>uv run ty check apps/</code> to see violations 3. Fix each violation (real fix or <code># ty: ignore[rule-name]</code> for false positives) 4. Verify <code>uv run ty check apps/</code> passes 5. Commit: <code>git commit -am \"types: enable &lt;rule-name&gt; and fix N violations\"</code></p> <p>Group rules into a single PR when combined violations are &lt;20.</p>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-8-enable-phase-2-medium-noise-rules-one-rule-per-pr","title":"Task 8: Enable Phase 2 medium-noise rules \u2014 one rule per PR","text":"<p>Each of these rules has 30-110 violations. Handle them as separate PRs.</p> <p>Process for each rule (repeat for each):</p> <p>Rules to enable in this order: 1. <code>unsupported-operator</code> (~31 violations) 2. <code>invalid-return-type</code> (~33 violations) 3. <code>possibly-missing-attribute</code> (~43 violations) 4. <code>invalid-method-override</code> (~46 violations) 5. <code>invalid-parameter-default</code> (~79 violations) 6. <code>not-subscriptable</code> (~107 violations) 7. <code>invalid-assignment</code> (~109 violations)</p> <p>For each rule:</p> <p>Step 1: Assess violations</p> <pre><code>uv run ty check --ignore all --error &lt;rule-name&gt; apps/ 2&gt;&amp;1 | head -100\n</code></pre> <p>Review the violations. Categorize: - Real bugs to fix - Django false positives to suppress with <code># ty: ignore[&lt;rule-name&gt;]</code> - Third-party library false positives to suppress</p> <p>Step 2: Enable the rule in pyproject.toml</p> <pre><code>&lt;rule-name&gt; = \"error\"\n</code></pre> <p>Step 3: Fix all violations</p> <ul> <li>Fix real bugs</li> <li>Add <code># ty: ignore[&lt;rule-name&gt;]</code> for false positives</li> <li>Never use blanket <code># type: ignore</code></li> </ul> <p>Step 4: Verify clean</p> <p><pre><code>uv run ty check apps/\n</code></pre> Expected: <code>All checks passed!</code></p> <p>Step 5: Commit and create PR</p> <pre><code>git add -u\ngit commit -m \"types: enable &lt;rule-name&gt; and fix N violations\"\n</code></pre> <p>PR title: <code>types: enable ty rule &lt;rule-name&gt;</code> PR description should list the violation count and categorize fixes vs suppressions.</p>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-9-enable-phase-3-high-noise-rules","title":"Task 9: Enable Phase 3 high-noise rules","text":"<p>These rules have the most violations. Handle them one at a time, potentially splitting into multiple commits within a PR.</p> <p>Rules: 1. <code>invalid-argument-type</code> (~369 violations) 2. <code>invalid-type-form</code> (~29 violations) 3. <code>unresolved-attribute</code> (~2,417 violations pre-django-types; re-measure)</p> <p>For <code>invalid-argument-type</code> and <code>invalid-type-form</code>:</p> <p>Follow the same process as Task 8. For large violation counts, split fixes by app directory:</p> <pre><code># Check per-app violation count\nfor app in apps/*/; do\n  count=$(uv run ty check --ignore all --error invalid-argument-type \"$app\" 2&gt;&amp;1 | grep -oP 'Found \\K\\d+' || echo \"0\")\n  [ \"$count\" != \"0\" ] &amp;&amp; echo \"$count $app\"\ndone\n</code></pre> <p>Consider splitting into multiple commits (one per app) within the same PR.</p> <p>For <code>unresolved-attribute</code>:</p> <p>This is the largest rule. After django-types is installed (Task 1), re-measure:</p> <pre><code>uv run ty check --ignore all --error unresolved-attribute apps/ 2&gt;&amp;1 | tail -3\n</code></pre> <p>If still &gt;500 violations, consider: - Enable as <code>\"warn\"</code> first instead of <code>\"error\"</code> - Fix app-by-app in separate PRs - Some may require upstream django-types fixes</p>"},{"location":"plans/2026-02-18-ty-type-checking-plan/#task-10-make-ci-blocking","title":"Task 10: Make CI blocking","text":"<p>Files: - Modify: <code>.github/workflows/lint_and_test.yml</code></p> <p>Prerequisite: All rules from Phases 1-3 that you plan to enforce are enabled and passing.</p> <p>Step 1: Remove continue-on-error</p> <p>In <code>.github/workflows/lint_and_test.yml</code>, change the <code>type-check</code> job:</p> <pre><code>  type-check:\n    if: github.ref != 'refs/heads/main'\n    runs-on: ubuntu-latest\n    # Remove: continue-on-error: true\n    steps:\n      ...\n</code></pre> <p>Step 2: Verify CI passes on current main</p> <p>Push a test branch and confirm the type-check job passes.</p> <p>Step 3: Commit</p> <pre><code>git add .github/workflows/lint_and_test.yml\ngit commit -m \"ci: make ty type checking blocking in CI\"\n</code></pre>"},{"location":"plans/human-annotations/","title":"Human Annotations Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Build a structured human review system where teams can create annotation queues, assign reviewers to sessions/messages, collect structured feedback, and surface patterns across many reviews.</p> <p>Architecture: A dedicated <code>human_annotations</code> Django app with <code>AnnotationQueue</code> (schema + reviewer config), <code>AnnotationItem</code> (session or message to review), <code>Annotation</code> (one reviewer's submission), and <code>AnnotationQueueAggregate</code> (aggregated results). Reuses the <code>FieldDefinition</code> schema pattern from the evaluations app. All models are team-scoped via <code>BaseTeamModel</code>.</p> <p>Tech Stack: Django, Django-tables2, Alpine.js, HTMX, Celery + Redis (async tasks), <code>apps/ocs_notifications</code> (notifications), <code>apps/events</code> (session lifecycle)</p>"},{"location":"plans/human-annotations/#status","title":"Status","text":""},{"location":"plans/human-annotations/#phase-1-complete-prs-2828-2846","title":"Phase 1 \u2014 Complete (PRs #2828, #2846)","text":"<p>The core infrastructure is fully implemented:</p> <ul> <li>Queue CRUD \u2014 Create/edit/delete queues with customizable JSON schemas (int, float, choice, string fields). Schema is locked once annotations begin (only <code>required</code> can change).</li> <li>Item management \u2014 Bulk-add sessions to a queue via <code>AddSessionsToQueue</code>; duplicate prevention; flag/unflag items with reasons.</li> <li>Annotation workflow \u2014 Assignees get the next unreviewed item; one review per reviewer per item; DRAFT \u2192 SUBMITTED lifecycle; automatic status updates when <code>num_reviews_required</code> is met.</li> <li>Aggregation \u2014 <code>compute_aggregates_for_queue()</code> runs on every submission; results stored in <code>AnnotationQueueAggregate</code>; text fields excluded.</li> <li>Export \u2014 CSV and JSONL export of all submitted annotations.</li> <li>Permissions \u2014 Queue management gated on team membership; annotation gated on assignee list.</li> </ul> <p>Key files: - Models: <code>apps/human_annotations/models.py</code> - Queue views: <code>apps/human_annotations/views/queue_views.py</code> - Annotation views: <code>apps/human_annotations/views/annotate_views.py</code> - Forms: <code>apps/human_annotations/forms.py</code> - Tests: <code>apps/human_annotations/tests/</code></p>"},{"location":"plans/human-annotations/#phase-2-item-9-complete-pr-2917","title":"Phase 2 \u2014 Item 9 Complete (PR #2917)","text":"<ul> <li>Session UI Integration \u2014 \"Add to Queue\" button on session detail page. HTMX modal lists active queues; POST creates <code>AnnotationItem</code>; already-queued sessions shown as disabled. Queue memberships displayed as badges alongside the button. Gated behind the <code>flag_human_annotations</code> feature flag (view raises 404 and template hides the block when inactive). Fixes applied post-dogfood: modal auto-opens via Alpine <code>x-init</code>, queue name badges appear on reload, UUID text truncated in items table.</li> </ul>"},{"location":"plans/human-annotations/#phase-2-remaining-work","title":"Phase 2 \u2014 Remaining Work","text":""},{"location":"plans/human-annotations/#item-9-session-ui-integration-add-to-queue-from-session-detail-done-pr-2917","title":"~~Item 9: Session UI Integration \u2014 Add to Queue from Session Detail~~ \u2713 Done (PR #2917)","text":"<p>Goal: Let a reviewer add the current session to an annotation queue directly from the session detail page, without navigating to the queue first.</p> <p>Context: - Session detail view: <code>apps/chatbots/views.py:530</code> (<code>chatbot_session_details_view</code>) - Rendered via <code>apps/generics/views.py:92</code> (<code>render_session_details</code>) - Session detail component template: <code>templates/experiments/components/experiment_details.html</code> - Existing bulk-add view (queue-side): <code>apps/human_annotations/views/queue_views.py:159</code> (<code>AddSessionsToQueue</code>) - URL namespace: <code>human_annotations</code></p> <p>What to build:</p> <ol> <li>New view <code>AddSessionToQueueFromSession</code> in <code>apps/human_annotations/views/queue_views.py</code>:</li> <li>GET: Returns an HTMX partial listing active queues for the team (exclude queues where session already exists)</li> <li>POST: Accepts <code>queue_id</code>; creates <code>AnnotationItem(queue=queue, item_type=SESSION, session=session)</code>; returns a success/already-added response partial</li> <li> <p>URL pattern: <code>GET/POST /sessions/&lt;str:session_id&gt;/add-to-queue/</code> in <code>apps/human_annotations/urls.py</code></p> </li> <li> <p>UI button in <code>templates/experiments/components/experiment_details.html</code>:</p> </li> <li>\"Add to Annotation Queue\" button with <code>hx-get</code> triggering the modal partial</li> <li> <p>On success, display inline confirmation (\"Added to [queue name]\" or \"Already in queue\")</p> </li> <li> <p>HTMX modal partial at <code>templates/human_annotations/add_session_to_queue_modal.html</code>:</p> </li> <li>List of active queues with radio or dropdown selection</li> <li>Submit button; cancel closes modal</li> <li>Show which queues already contain this session (disabled/greyed)</li> </ol> <p>Permissions: User must be a team member; queue must be ACTIVE and belong to the same team.</p> <p>Tests: Add to <code>apps/human_annotations/tests/test_views.py</code>: - Session not in any queue \u2192 shows all active queues - Session already in queue X \u2192 queue X shown as disabled - POST adds item and returns 200 - POST with duplicate session returns appropriate message - Non-team-member cannot access view</p>"},{"location":"plans/human-annotations/#item-10-queue-automation-auto-add-sessions-based-on-criteria","title":"Item 10: Queue Automation \u2014 Auto-Add Sessions Based on Criteria","text":"<p>Goal: Let queue owners define filter criteria so that sessions are automatically added to an annotation queue when they match (e.g., by experiment, tag, participant, or date).</p> <p>Context: - <code>AnnotationQueue</code> model: <code>apps/human_annotations/models.py</code> - <code>ExperimentSession</code> model: <code>apps/experiments/models/experiment_models.py</code> - Celery tasks pattern: see <code>apps/evaluations/tasks.py</code> or <code>apps/events/tasks.py</code> for examples - Events/signals for session lifecycle: no Django signals currently; <code>EventLog</code> in <code>apps/events/models.py</code> tracks session events</p> <p>What to build:</p> <ol> <li> <p><code>criteria</code> field on <code>AnnotationQueue</code> \u2014 a <code>SanitizedJSONField</code> (nullable) storing a dict of filter criteria. Supported keys:    <pre><code>{\n  \"experiments\": [\"&lt;uuid&gt;\", ...],\n  \"tags\": [\"tag-name\", ...],\n  \"participants\": [\"external_id\", ...],\n  \"session_status\": \"completed\"\n}\n</code></pre>    Migration: <code>apps/human_annotations/migrations/0003_annotationqueue_criteria.py</code></p> </li> <li> <p><code>evaluate_criteria(queue, session) -&gt; bool</code> function in <code>apps/human_annotations/criteria.py</code>:</p> </li> <li>Takes a queue and a session; returns True if session matches all defined criteria</li> <li> <p>Each criterion is ANDed; empty/null criteria matches nothing (automation is opt-in)</p> </li> <li> <p>Celery task <code>auto_add_sessions_to_queues</code> in <code>apps/human_annotations/tasks.py</code>:</p> </li> <li>Periodic task (e.g., every 15 minutes, or triggered on-demand)</li> <li>For each ACTIVE queue with non-null criteria, query matching sessions, bulk-create missing <code>AnnotationItem</code> records</li> <li> <p>Use <code>bulk_create(ignore_conflicts=True)</code> (already the pattern in <code>AddSessionsToQueue</code>)</p> </li> <li> <p>Signal hook (optional, for immediate ingestion):</p> </li> <li>In <code>apps/human_annotations/apps.py</code>, connect a <code>post_save</code> signal on <code>ExperimentSession</code></li> <li>On session status change to \"completed\", check all active queues with criteria for this team</li> <li> <p>Only do lightweight criteria evaluation in-process; offload bulk ops to Celery</p> </li> <li> <p>UI in queue form (<code>templates/human_annotations/queue_form.html</code>):</p> </li> <li>Collapsible \"Automation\" section after the schema builder</li> <li>Experiment multi-select (Alpine-powered), tag input, session status dropdown</li> <li>Save criteria alongside the queue (handled by <code>AnnotationQueueForm</code>)</li> </ol> <p>Tests: - <code>evaluate_criteria</code> correctly filters by experiment, tag, participant, status - <code>evaluate_criteria</code> returns False for null/empty criteria - Celery task creates items for matching sessions, skips duplicates - Queue form saves and reloads criteria correctly - Signal hook enqueues task when session completes (mock Celery)</p>"},{"location":"plans/human-annotations/#item-11-notification-integration","title":"Item 11: Notification Integration","text":"<p>Goal: Notify relevant users when annotation queue activity occurs: new items added, items flagged, and review completion.</p> <p>Context: - Notification infrastructure: <code>apps/ocs_notifications/</code> - Core utility: <code>apps/ocs_notifications/utils.py</code> \u2192 <code>create_notification(title, message, level, team, slug, event_data, permissions, links)</code> - Predefined notification creators: <code>apps/ocs_notifications/notifications.py</code> - User notification preferences model: <code>apps/ocs_notifications/models.py</code> (<code>UserNotificationPreferences</code>) - Notification levels: INFO, WARNING, ERROR</p> <p>Three notification events to implement:</p>"},{"location":"plans/human-annotations/#11a-new-items-added-to-a-queue","title":"11a. New items added to a queue","text":"<ul> <li>Trigger: After <code>AnnotationItem</code> records are created (bulk or single)</li> <li>Recipients: Assignees of the queue</li> <li>Message: \"New items added to annotation queue '[queue name]' \u2014 [N] items await review.\"</li> <li>Level: INFO</li> <li>Links: Link to <code>queue:annotate</code> (the \"get next item\" URL for that queue)</li> </ul>"},{"location":"plans/human-annotations/#11b-item-flagged","title":"11b. Item flagged","text":"<ul> <li>Trigger: <code>FlagItem</code> view (POST to <code>/item/&lt;item_pk&gt;/flag/</code>)</li> <li>Recipients: Queue creator (<code>queue.created_by</code>) + team admins</li> <li>Message: \"Item flagged in queue '[queue name]': [flag reason]\"</li> <li>Level: WARNING</li> <li>Links: Link to the specific item's annotation view</li> </ul>"},{"location":"plans/human-annotations/#11c-queue-completed-all-items-reviewed","title":"11c. Queue completed (all items reviewed)","text":"<ul> <li>Trigger: After <code>AnnotationItem.update_status()</code> when queue progress hits 100% (all items COMPLETED)</li> <li>Recipients: Queue creator</li> <li>Message: \"Annotation queue '[queue name]' is complete \u2014 all [N] items have been reviewed.\"</li> <li>Level: INFO</li> <li>Links: Link to queue detail / export</li> </ul> <p>What to build:</p> <ol> <li> <p><code>apps/human_annotations/notifications.py</code> \u2014 module with three functions:    <pre><code>def notify_items_added(queue: AnnotationQueue, count: int) -&gt; None: ...\ndef notify_item_flagged(item: AnnotationItem, reason: str, flagging_user) -&gt; None: ...\ndef notify_queue_completed(queue: AnnotationQueue) -&gt; None: ...\n</code></pre>    Each calls <code>create_notification()</code> from <code>apps/ocs_notifications/utils.py</code>.</p> </li> <li> <p>Hook into existing views/models:</p> </li> <li><code>AddSessionsToQueue.post()</code> and <code>AddSessionToQueueFromSession.post()</code> \u2192 call <code>notify_items_added</code> after bulk_create</li> <li><code>FlagItem.post()</code> \u2192 call <code>notify_item_flagged</code></li> <li> <p><code>AnnotationQueue.get_progress()</code> or <code>AnnotationItem.update_status()</code> \u2192 call <code>notify_queue_completed</code> when newly completed (guard against repeat notifications)</p> </li> <li> <p>Unique <code>slug</code> identifiers for notification deduplication:</p> </li> <li>Items added: <code>annotation_queue_items_added</code></li> <li>Item flagged: <code>annotation_item_flagged</code></li> <li>Queue completed: <code>annotation_queue_completed</code></li> </ol> <p>Tests: - Calling <code>notify_items_added</code> creates a <code>NotificationEvent</code> for each assignee - Calling <code>notify_item_flagged</code> creates notification for queue creator - <code>notify_queue_completed</code> fires once when all items are COMPLETED, not on subsequent saves - Users with notifications disabled do not receive emails (respect <code>UserNotificationPreferences</code>)</p>"},{"location":"plans/human-annotations/#future-items-not-in-scope-for-phase-2","title":"Future Items (not in scope for Phase 2)","text":"<ul> <li>Item 7: Quality metrics \u2014 inter-reviewer agreement (Cohen's kappa, Krippendorff's alpha) computed and surfaced on queue detail page</li> <li>Item 8: Evaluation workflow integration \u2014 trigger annotation queues from evaluator results; link <code>Annotation</code> data back to evaluation runs</li> </ul>"},{"location":"plans/human-annotations/#implementation-order","title":"Implementation Order","text":"<p>Recommended sequence: 1. Item 9 (Session UI) \u2014 highest user-facing value, self-contained, no new models 2. Item 11 (Notifications) \u2014 hooks into existing views, low-risk 3. Item 10 (Automation) \u2014 most complex; requires new model field, migration, and Celery task</p> <p>Each item should be implemented with TDD (write failing test \u2192 implement \u2192 green) and committed independently.</p>"}]}