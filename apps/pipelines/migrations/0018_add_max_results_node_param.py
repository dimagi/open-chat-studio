# Generated by Django 5.1.5 on 2025-06-11 14:29

from django.db import migrations
from apps.pipelines.nodes.nodes import LLMResponseWithPrompt

def _batched_param_update(Node, update_func: callable, chunk_size=100):
    queryset = Node.objects.filter(type=LLMResponseWithPrompt.__name__).iterator(chunk_size=chunk_size)
    nodes_to_update = []

    counter = 0
    for idx, node in enumerate(queryset):
        node.params = update_func(node.params)
        nodes_to_update.append(node)
        counter += 1
        
        if idx % chunk_size == 0 and idx > 0:
            Node.objects.bulk_update(nodes_to_update, ['params'])
            nodes_to_update = []
            counter = 0
    
    # Update any remaining nodes that were not updated in the last batch
    if nodes_to_update:
        Node.objects.bulk_update(nodes_to_update, ['params'])

def _populate_max_results_for_llm_nodes(apps, schema_editor):
    Node = apps.get_model('pipelines', 'Node')

    def _add_max_results_param(param):
        if 'max_results' not in param:
            param['max_results'] = 20
        return param

    _batched_param_update(Node, update_func=_add_max_results_param)

def _remove_max_results_from_llm_nodes(apps, schema_editor):
    Node = apps.get_model('pipelines', 'Node')

    def _remove_max_results_param(param):
        param.pop('max_results', None)
        return param
    
    _batched_param_update(Node, update_func=_remove_max_results_param)


class Migration(migrations.Migration):

    dependencies = [
        ('pipelines', '0017_pipelinechatmessages_compression_marker'),
    ]

    operations = [
        migrations.RunPython(_populate_max_results_for_llm_nodes, _remove_max_results_from_llm_nodes),
    ]
