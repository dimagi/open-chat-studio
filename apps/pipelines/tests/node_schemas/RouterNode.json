{
  "description": "Routes the input to one of the linked nodes using an LLM",
  "properties": {
    "llm_provider_id": {
      "title": "LLM Model",
      "type": "integer",
      "ui:widget": "llm_provider_model"
    },
    "llm_provider_model_id": {
      "title": "Llm Provider Model Id",
      "type": "integer",
      "ui:widget": "none"
    },
    "llm_model_parameters": {
      "additionalProperties": true,
      "title": "Llm Model Parameters",
      "type": "object",
      "ui:widget": "none"
    },
    "history_type": {
      "enum": [
        "node",
        "named",
        "global",
        "none"
      ],
      "title": "PipelineChatHistoryTypes",
      "type": "string",
      "default": "node",
      "ui:enumLabels": [
        "Node",
        "Named",
        "Global",
        "No History"
      ],
      "ui:widget": "history"
    },
    "history_name": {
      "default": null,
      "title": "History Name",
      "ui:widget": "none",
      "type": "string"
    },
    "history_mode": {
      "enum": [
        "summarize",
        "truncate_tokens",
        "max_history_length"
      ],
      "title": "PipelineChatHistoryModes",
      "type": "string",
      "default": "summarize",
      "ui:enumLabels": [
        "Summarize",
        "Truncate Tokens",
        "Max History Length"
      ],
      "ui:widget": "history_mode"
    },
    "user_max_token_limit": {
      "default": null,
      "description": "Maximum number of tokens before messages are summarized or truncated.",
      "title": "Token Limit",
      "ui:visibleWhen": {
        "field": "history_mode",
        "operator": "in",
        "value": [
          "summarize",
          "truncate_tokens"
        ]
      },
      "type": "integer"
    },
    "max_history_length": {
      "default": 10,
      "description": "Chat history will only keep the most recent messages up to this limit.",
      "title": "Max History Length",
      "type": "integer",
      "ui:visibleWhen": {
        "field": "history_mode",
        "operator": "==",
        "value": "max_history_length"
      }
    },
    "name": {
      "title": "Node Name",
      "type": "string",
      "ui:widget": "node_name"
    },
    "keywords": {
      "items": {
        "type": "string"
      },
      "title": "Keywords",
      "type": "array",
      "ui:widget": "keywords"
    },
    "default_keyword_index": {
      "default": 0,
      "title": "Default Keyword Index",
      "type": "integer",
      "ui:widget": "none"
    },
    "tag_output_message": {
      "default": false,
      "description": "Tag the output message with the selected route",
      "title": "Tag Output Message",
      "type": "boolean",
      "ui:widget": "toggle"
    },
    "prompt": {
      "default": "You are an extremely helpful router",
      "minLength": 1,
      "title": "Prompt",
      "type": "string",
      "ui:optionsSource": "text_editor_autocomplete_vars_router_node",
      "ui:widget": "text_editor_widget"
    }
  },
  "required": [
    "llm_provider_id",
    "llm_provider_model_id",
    "name"
  ],
  "title": "RouterNode",
  "type": "object",
  "ui:can_add": true,
  "ui:can_delete": true,
  "ui:deprecated": false,
  "ui:documentation_link": "/concepts/pipelines/nodes/#llm-router",
  "ui:flow_node_type": "pipelineNode",
  "ui:icon": "fa-solid fa-arrows-split-up-and-left",
  "ui:label": "LLM Router",
  "ui:order": [
    "llm_provider_id",
    "llm_temperature",
    "history_type",
    "prompt",
    "keywords",
    "tag_output_message"
  ]
}
